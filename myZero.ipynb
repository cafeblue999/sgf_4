{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "myZero.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cafeblue999/sgf_4/blob/master/myZero.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1WJzOAusglq",
        "colab_type": "code",
        "outputId": "a5125f19-37bf-4e2b-8055-b3e12b8604ca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5647
        }
      },
      "source": [
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from sys import stderr, stdout, exit\n",
        "from datetime import datetime, date, timedelta\n",
        "import numpy as np\n",
        "import logging\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import time\n",
        "from collections import Counter\n",
        "import random\n",
        "import os\n",
        "\n",
        "from keras.layers import *\n",
        "from keras.models import Model\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "basename = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_file = 'log_' + basename + '.txt'\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s\\t%(levelname)s\\t%(message)s', datefmt='%Y/%m/%d %H:%M:%S', filename=log_file, level=logging.INFO)\n",
        "\n",
        "def stdout_log(str):\n",
        "    now_dt = datetime.now()\n",
        "    dt_jst = now_dt + timedelta(minutes=540)\n",
        "    str_datetime = datetime.strftime(dt_jst, \"%Y/%m/%d-%H:%M:%S\")\n",
        "    stdout.write(str_datetime + \" \" + str + \"\\n\")\n",
        "    #log_file = open(\"log.txt\", \"a\")\n",
        "    #log_file.write(str)\n",
        "    #log_file.close()\n",
        "\n",
        "    \n",
        "BSIZE = 19  # board size\n",
        "EBSIZE = BSIZE + 2  # extended board size\n",
        "BVCNT = BSIZE ** 2  # vertex count\n",
        "EBVCNT = EBSIZE ** 2  # extended vertex count\n",
        "PASS = EBVCNT  # pass\n",
        "VNULL = EBVCNT + 1  # invalid position\n",
        "KOMI = 6.5\n",
        "dir4 = [1, EBSIZE, -1, -EBSIZE]\n",
        "diag4 = [1 + EBSIZE, EBSIZE - 1, -EBSIZE - 1, 1 - EBSIZE]\n",
        "KEEP_PREV_CNT = 7\n",
        "FEATURE_CNT = KEEP_PREV_CNT * 2 + 3  # 7\n",
        "x_labels = \"ABCDEFGHJKLMNOPQRST\"\n",
        "\n",
        "\n",
        "def ev2xy(ev):\n",
        "    return ev % EBSIZE, ev // EBSIZE\n",
        "\n",
        "def xy2ev(x, y):\n",
        "    return y * EBSIZE + x\n",
        "\n",
        "def rv2ev(rv):\n",
        "    if rv == BVCNT:\n",
        "        return PASS\n",
        "    return rv % BSIZE + 1 + (rv // BSIZE + 1) * EBSIZE\n",
        "\n",
        "def ev2rv(ev):\n",
        "    if ev == PASS:\n",
        "        return BVCNT\n",
        "    return ev % EBSIZE - 1 + (ev // EBSIZE - 1) * BSIZE\n",
        "\n",
        "def ev2str(ev):\n",
        "    if ev >= PASS:        \n",
        "        return \"PASS\"\n",
        "    x, y = ev2xy(ev)\n",
        "    return x_labels[x - 1] + str(y)\n",
        "\n",
        "def str2ev(v_str):\n",
        "    v_str = v_str.upper()\n",
        "    if v_str == \"PASS\" or v_str == \"RESIGN\":\n",
        "        return PASS\n",
        "    else:\n",
        "        x = x_labels.find(v_str[0]) + 1\n",
        "        y = int(v_str[1:])\n",
        "        return xy2ev(x, y)\n",
        "\n",
        "def turn2str(turn):\n",
        "    if turn == 0:\n",
        "        return 'W'\n",
        "    else:\n",
        "        return 'B'\n",
        "\n",
        "rv_list = [rv2ev(i) for i in range(BVCNT)]\n",
        "\n",
        "\n",
        "class StoneGroup(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.lib_cnt = VNULL  # liberty count\n",
        "        self.size = VNULL  # stone size\n",
        "        self.v_atr = VNULL  # liberty position if in Atari\n",
        "        self.libs = set()  # set of liberty positions\n",
        "\n",
        "    def clear(self, stone=True):\n",
        "        # clear as placed stone or empty\n",
        "        self.lib_cnt = 0 if stone else VNULL\n",
        "        self.size = 1 if stone else VNULL\n",
        "        self.v_atr = VNULL\n",
        "        self.libs.clear()\n",
        "\n",
        "    def add(self, v):\n",
        "        # add liberty at v\n",
        "        if v not in self.libs:\n",
        "            self.libs.add(v)\n",
        "            self.lib_cnt += 1\n",
        "            self.v_atr = v\n",
        "\n",
        "    def sub(self, v):\n",
        "        # remove liberty at v\n",
        "        if v in self.libs:\n",
        "            self.libs.remove(v)\n",
        "            self.lib_cnt -= 1\n",
        "\n",
        "    def merge(self, other):\n",
        "        # merge with aother stone group\n",
        "        self.libs |= other.libs\n",
        "        self.lib_cnt = len(self.libs)\n",
        "        self.size += other.size\n",
        "        if self.lib_cnt == 1:\n",
        "            for lib in self.libs:\n",
        "                self.v_atr = lib\n",
        "\n",
        "\n",
        "class Board(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # 1-d array ([EBVCNT]) of stones or empty or exterior\n",
        "        # 0: white 1: black\n",
        "        # 2: empty 3: exterior\n",
        "        self.color = np.full(EBVCNT, 3)\n",
        "        self.sg = [StoneGroup() for _ in range(EBVCNT)]  # stone groups\n",
        "        self.clear()\n",
        "\n",
        "    def clear(self):\n",
        "        self.color[rv_list] = 2  # empty\n",
        "        self.id = np.arange(EBVCNT)  # id of stone group\n",
        "        self.next = np.arange(EBVCNT)  # next position in the same group\n",
        "        for i in range(EBVCNT):\n",
        "            self.sg[i].clear(stone=False)\n",
        "        self.prev_color = [np.copy(self.color) for _ in range(KEEP_PREV_CNT)]\n",
        "\n",
        "        self.ko = VNULL  # illegal position due to Ko\n",
        "        self.turn = 1  # black\n",
        "        self.move_cnt = 0  # move count\n",
        "        self.prev_move = VNULL  # previous move\n",
        "        self.remove_cnt = 0  # removed stones count\n",
        "        self.history = []\n",
        "\n",
        "    def copy(self, b_cpy):\n",
        "        b_cpy.color = np.copy(self.color)\n",
        "        b_cpy.id = np.copy(self.id)\n",
        "        b_cpy.next = np.copy(self.next)\n",
        "        for i in range(EBVCNT):\n",
        "            b_cpy.sg[i].lib_cnt = self.sg[i].lib_cnt\n",
        "            b_cpy.sg[i].size = self.sg[i].size\n",
        "            b_cpy.sg[i].v_atr = self.sg[i].v_atr\n",
        "            b_cpy.sg[i].libs |= self.sg[i].libs\n",
        "        for i in range(KEEP_PREV_CNT):\n",
        "            b_cpy.prev_color[i] = np.copy(self.prev_color[i])\n",
        "\n",
        "        b_cpy.ko = self.ko\n",
        "        b_cpy.turn = self.turn\n",
        "        b_cpy.move_cnt = self.move_cnt\n",
        "        b_cpy.prev_move = self.prev_move\n",
        "        b_cpy.remove_cnt = self.remove_cnt\n",
        "\n",
        "        for h in self.history:\n",
        "            b_cpy.history.append(h)\n",
        "\n",
        "    def remove(self, v):\n",
        "        # remove stone group including stone at v\n",
        "        v_tmp = v\n",
        "        while 1:\n",
        "            self.remove_cnt += 1\n",
        "            self.color[v_tmp] = 2  # empty\n",
        "            self.id[v_tmp] = v_tmp  # reset id\n",
        "            for d in dir4:\n",
        "                nv = v_tmp + d\n",
        "                # add liberty to neighbor groups\n",
        "                self.sg[self.id[nv]].add(v_tmp)\n",
        "            v_next = self.next[v_tmp]\n",
        "            self.next[v_tmp] = v_tmp\n",
        "            v_tmp = v_next\n",
        "            if v_tmp == v:\n",
        "                break  # finish when all stones are removed\n",
        "\n",
        "    def merge(self, v1, v2):\n",
        "        # merge stone groups at v1 and v2\n",
        "        id_base = self.id[v1]\n",
        "        id_add = self.id[v2]\n",
        "        if self.sg[id_base].size < self.sg[id_add].size:\n",
        "            id_base, id_add = id_add, id_base  # swap\n",
        "        self.sg[id_base].merge(self.sg[id_add])\n",
        "\n",
        "        v_tmp = id_add\n",
        "        while 1:\n",
        "            self.id[v_tmp] = id_base  # change id to id_base\n",
        "            v_tmp = self.next[v_tmp]\n",
        "            if v_tmp == id_add:\n",
        "                break\n",
        "        # swap next id for circulation\n",
        "        self.next[v1], self.next[v2] = self.next[v2], self.next[v1]\n",
        "\n",
        "    def place_stone(self, v):\n",
        "        self.color[v] = self.turn\n",
        "        self.id[v] = v\n",
        "        self.sg[self.id[v]].clear(stone=True)\n",
        "        for d in dir4:\n",
        "            nv = v + d\n",
        "            if self.color[nv] == 2:\n",
        "                self.sg[self.id[v]].add(nv)  # add liberty\n",
        "            else:\n",
        "                self.sg[self.id[nv]].sub(v)  # remove liberty\n",
        "        # merge stone groups\n",
        "        for d in dir4:\n",
        "            nv = v + d\n",
        "            if self.color[nv] == self.turn and self.id[nv] != self.id[v]:\n",
        "                self.merge(v, nv)\n",
        "        # remove opponent's stones\n",
        "        self.remove_cnt = 0\n",
        "        for d in dir4:\n",
        "            nv = v + d\n",
        "            if self.color[nv] == int(self.turn == 0) and \\\n",
        "                    self.sg[self.id[nv]].lib_cnt == 0:\n",
        "                self.remove(nv)\n",
        "\n",
        "    def legal(self, v):\n",
        "        if v == PASS:\n",
        "            return True\n",
        "        elif v == self.ko or self.color[v] != 2:\n",
        "            return False\n",
        "\n",
        "        stone_cnt = [0, 0]\n",
        "        atr_cnt = [0, 0]\n",
        "        for d in dir4:\n",
        "            nv = v + d\n",
        "            c = self.color[nv]\n",
        "            if c == 2:\n",
        "                return True\n",
        "            elif c <= 1:\n",
        "                stone_cnt[c] += 1\n",
        "                if self.sg[self.id[nv]].lib_cnt == 1:\n",
        "                    atr_cnt[c] += 1\n",
        "\n",
        "        return (atr_cnt[int(self.turn == 0)] != 0 or\n",
        "                atr_cnt[self.turn] < stone_cnt[self.turn])\n",
        "\n",
        "    def eyeshape(self, v, pl):\n",
        "        if v == PASS:\n",
        "            return False\n",
        "        for d in dir4:\n",
        "            c = self.color[v + d]\n",
        "            if c == 2 or c == int(pl == 0):\n",
        "                return False\n",
        "\n",
        "        diag_cnt = [0, 0, 0, 0]\n",
        "        for d in diag4:\n",
        "            nv = v + d\n",
        "            diag_cnt[self.color[nv]] += 1\n",
        "\n",
        "        wedge_cnt = diag_cnt[int(pl == 0)] + int(diag_cnt[3] > 0)\n",
        "        if wedge_cnt == 2:\n",
        "            for d in diag4:\n",
        "                nv = v + d\n",
        "                if self.color[nv] == int(pl == 0) and \\\n",
        "                        self.sg[self.id[nv]].lib_cnt == 1 and \\\n",
        "                        self.sg[self.id[nv]].v_atr != self.ko:\n",
        "                    return True\n",
        "\n",
        "        return wedge_cnt < 2\n",
        "\n",
        "    def play(self, v, not_fill_eye=True):\n",
        "\n",
        "        #logging.info(\"board play : {}\".format(ev2str(v)))\n",
        "\n",
        "        if not self.legal(v):\n",
        "            logging.info(\"board play : not legal = {}\".format(ev2str(v)))\n",
        "            return 1\n",
        "        elif not_fill_eye and self.eyeshape(v, self.turn):\n",
        "            return 2\n",
        "        else:\n",
        "            for i in range(KEEP_PREV_CNT - 1)[::-1]:\n",
        "                self.prev_color[i + 1] = np.copy(self.prev_color[i])\n",
        "            self.prev_color[0] = np.copy(self.color)\n",
        "\n",
        "            if v == PASS:\n",
        "                self.ko = VNULL\n",
        "            else:\n",
        "                self.place_stone(v)\n",
        "                id = self.id[v]\n",
        "                self.ko = VNULL\n",
        "                if self.remove_cnt == 1 and \\\n",
        "                        self.sg[id].lib_cnt == 1 and \\\n",
        "                        self.sg[id].size == 1:\n",
        "                    self.ko = self.sg[id].v_atr\n",
        "\n",
        "        self.prev_move = v\n",
        "        self.history.append(v)\n",
        "        self.turn = int(self.turn == 0)\n",
        "        self.move_cnt += 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def random_play(self):\n",
        "        empty_list = np.where(self.color == 2)[0]\n",
        "        np.random.shuffle(empty_list)\n",
        "\n",
        "        for v in empty_list:\n",
        "            if self.play(v, True) == 0:\n",
        "                return v\n",
        "\n",
        "        self.play(PASS)\n",
        "        return PASS\n",
        "\n",
        "    def score(self):\n",
        "        stone_cnt = [0, 0]\n",
        "        for rv in range(BVCNT):\n",
        "            v = rv2ev(rv)\n",
        "            c = self.color[v]\n",
        "            if c <= 1:\n",
        "                stone_cnt[c] += 1\n",
        "            else:\n",
        "                nbr_cnt = [0, 0, 0, 0]\n",
        "                for d in dir4:\n",
        "                    nbr_cnt[self.color[v + d]] += 1\n",
        "                if nbr_cnt[0] > 0 and nbr_cnt[1] == 0:\n",
        "                    stone_cnt[0] += 1\n",
        "                elif nbr_cnt[1] > 0 and nbr_cnt[0] == 0:\n",
        "                    stone_cnt[1] += 1\n",
        "        return stone_cnt[1] - stone_cnt[0] - KOMI\n",
        "\n",
        "    def rollout(self, show_board=False):\n",
        "        while self.move_cnt < EBVCNT * 2:\n",
        "            prev_move = self.prev_move\n",
        "            move = self.random_play()\n",
        "            if show_board and move != PASS:\n",
        "                stderr.write(\"\\nmove count=%d\\n\" % b.move_cnt)\n",
        "                b.showboard()\n",
        "            if prev_move == PASS and move == PASS:\n",
        "                break\n",
        "\n",
        "    def showboard(self):\n",
        "\n",
        "        def pirnt_xlabel():\n",
        "            line_str = \"  \"\n",
        "            for x in range(BSIZE):\n",
        "                line_str += \" \" + x_labels[x] + \" \"\n",
        "            stderr.write(line_str + \"\\n\")\n",
        "\n",
        "        pirnt_xlabel()\n",
        "\n",
        "        for y in range(1, BSIZE + 1)[::-1]:  # 9, 8, ..., 1\n",
        "            line_str = str(y) if y >= 10 else \" \" + str(y)\n",
        "            for x in range(1, BSIZE + 1):\n",
        "                v = xy2ev(x, y)\n",
        "                x_str = \" . \"\n",
        "                color = self.color[v]\n",
        "                if color <= 1:\n",
        "                    stone_str = \"O\" if color == 0 else \"X\"\n",
        "                    if v == self.prev_move:\n",
        "                        x_str = \"[\" + stone_str + \"]\"\n",
        "                    else:\n",
        "                        x_str = \" \" + stone_str + \" \"\n",
        "                line_str += x_str\n",
        "            line_str += str(y) if y >= 10 else \" \" + str(y)\n",
        "            stderr.write(line_str + \"\\n\")\n",
        "\n",
        "        pirnt_xlabel()\n",
        "        stderr.write(\"\\n\")\n",
        "\n",
        "    def showboard_file(self):\n",
        "\n",
        "        def pirnt_xlabel():\n",
        "            line_str = \"  \"\n",
        "            for x in range(BSIZE):\n",
        "                line_str += \" \" + x_labels[x] + \" \"\n",
        "            #logging.info(line_str + \"\\n\")\n",
        "            logging.info(line_str)\n",
        "\n",
        "        pirnt_xlabel()\n",
        "\n",
        "        for y in range(1, BSIZE + 1)[::-1]:  # 9, 8, ..., 1\n",
        "            line_str = str(y) if y >= 10 else \" \" + str(y)\n",
        "            for x in range(1, BSIZE + 1):\n",
        "                v = xy2ev(x, y)\n",
        "                x_str = \" . \"\n",
        "                color = self.color[v]\n",
        "                if color <= 1:\n",
        "                    stone_str = \"O\" if color == 0 else \"x\"\n",
        "                    if v == self.prev_move:\n",
        "                        x_str = \"[\" + stone_str + \"]\"\n",
        "                    else:\n",
        "                        x_str = \" \" + stone_str + \" \"\n",
        "                line_str += x_str\n",
        "            line_str += str(y) if y >= 10 else \" \" + str(y)\n",
        "            #logging.info(line_str + \"\\n\")\n",
        "            logging.info(line_str)\n",
        "\n",
        "        pirnt_xlabel()\n",
        "        #logging.info(\"\\n\")\n",
        "        logging.info(\"\\n\")\n",
        "\n",
        "    def feature(self):\n",
        "        feature_ = np.zeros((EBVCNT, FEATURE_CNT), dtype=np.float)\n",
        "        my = self.turn\n",
        "        opp = int(self.turn == 0)\n",
        "\n",
        "        feature_[:, 0] = (self.color == my)\n",
        "        feature_[:, 1] = (self.color == opp)\n",
        "        for i in range(KEEP_PREV_CNT):\n",
        "            feature_[:, (i + 1) * 2] = (self.prev_color[i] == my)\n",
        "            feature_[:, (i + 1) * 2 + 1] = (self.prev_color[i] == opp)\n",
        "        feature_[:, FEATURE_CNT - 1] = my\n",
        "\n",
        "        return feature_[rv_list, :]\n",
        "\n",
        "    def hash(self):\n",
        "        return (hash(self.color.tostring()) ^\n",
        "                hash(self.prev_color[0].tostring()) ^ self.turn)\n",
        "\n",
        "    def info(self):\n",
        "        empty_list = np.where(self.color == 2)[0]\n",
        "        cand_list = []\n",
        "        for v in empty_list:\n",
        "            if self.legal(v) and not self.eyeshape(v, self.turn):\n",
        "                cand_list.append(ev2rv(v))\n",
        "        cand_list.append(ev2rv(PASS))\n",
        "        return (self.hash(), self.move_cnt, cand_list)\n",
        "\n",
        "      # -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "\n",
        "rnd_array = [np.arange(BVCNT + 1)]\n",
        "for i in range(1, 8):\n",
        "    rnd_array.append(rnd_array[i - 1])\n",
        "    rot_array = rnd_array[i][:BVCNT].reshape(BSIZE, BSIZE)\n",
        "    if i % 2 == 0:\n",
        "        rot_array = rot_array.transpose(1, 0)\n",
        "    else:\n",
        "        rot_array = rot_array[::-1, :]\n",
        "    rnd_array[i][:BVCNT] = rot_array.reshape(BVCNT)\n",
        "\n",
        "\n",
        "class Feed(object):\n",
        "\n",
        "    def __init__(self, f_, m_, r_):\n",
        "        self._feature = f_\n",
        "        self._move = m_\n",
        "        self._result = r_\n",
        "        self.size = self._feature.shape[0]\n",
        "        self._idx = 0\n",
        "        self._perm = np.arange(self.size)\n",
        "        np.random.shuffle(self._perm)\n",
        "\n",
        "        #logging.info(\"self._feature = {}\".format(f_))\n",
        "        logging.info(\"self._move shape = {}\".format(m_.shape))\n",
        "        #logging.info(\"self._move = {}\".format(m_))\n",
        "        logging.info(\"self._result shape = {}\".format(r_.shape))\n",
        "        #logging.info(\"self._result = {}\".format(r_))\n",
        "        logging.info(\"self.feature shape = {}\".format(self._feature.shape))\n",
        "        logging.info(\"self.size = {}\".format(self._feature.shape[0]))\n",
        "        logging.info(\"self._idx = {}\".format(self._idx))\n",
        "        logging.info(\"self._perm shape = {}\".format(self._perm.shape))\n",
        "        #logging.info(\"self._perm = {}\".format(self._perm))\n",
        "\n",
        "    def next_batch(self, batch_size=128):\n",
        "        if self._idx > self.size:\n",
        "            np.random.shuffle(self._perm)\n",
        "            self._idx = 0\n",
        "        start = self._idx\n",
        "        self._idx += batch_size\n",
        "        end = self._idx\n",
        "\n",
        "        #logging.info(\"start = {} end = {}\".format(start, end))\n",
        "        #logging.info(\"self._feature = {}\".format(self._feature))\n",
        "\n",
        "        rnd_cnt = np.random.choice(np.arange(8))\n",
        "\n",
        "        f_batch = self._feature[self._perm[start:end]]  # slice for mini-batch\n",
        "        #logging.info('f_batch_1 = {}'.format(f_batch))\n",
        "        \n",
        "        f_batch = f_batch[:, rnd_array[rnd_cnt][:BVCNT]].astype(np.float32)\n",
        "        #logging.info('f_batch_2 = {}'.format(f_batch))\n",
        "        \n",
        "        m_batch = self._move[self._perm[start:end]]  # slice for mini-batch\n",
        "        #logging.info('m_batch_1 = {}'.format(m_batch))\n",
        "        \n",
        "        m_batch = m_batch[:, rnd_array[rnd_cnt]].astype(np.float32)\n",
        "        #logging.info('f_batch_2 = {}'.format(m_batch))\n",
        "        \n",
        "        r_batch = self._result[self._perm[start:end]].astype(np.float32)\n",
        "        #logging.info('r_batch = {}'.format(r_batch))\n",
        "\n",
        "        return f_batch, m_batch, r_batch\n",
        "\n",
        "########################### Model #############################################    \n",
        "def dual_residual_network(input_shape, blocks=20):\n",
        "    stdout_log('- Model : dual_resnet input_shape = {}'.format(input_shape))\n",
        "    stdout_log('- Model : blocks = {}'.format(blocks))\n",
        "    inputs = Input(batch_shape=input_shape)\n",
        "    first_conv = conv_bn_relu_block(name=\"init\")(inputs)\n",
        "    res_tower = residual_tower(blocks=blocks)(first_conv)\n",
        "    policy = policy_head()(res_tower)\n",
        "    value = value_head()(res_tower)\n",
        "    return Model(inputs=inputs, outputs=[policy, value])\n",
        "\n",
        "def conv_bn_relu_block(name, activation=True, filters=256, kernel_size=(3,3), \n",
        "                       strides=(1,1), padding=\"same\", init=\"he_normal\"):\n",
        "    def f(inputs):\n",
        "        conv = Conv2D(filters=filters, \n",
        "                      kernel_size=kernel_size,\n",
        "                      strides=strides,\n",
        "                      padding=padding,\n",
        "                      kernel_initializer=init,\n",
        "                      data_format='channels_last',\n",
        "                      kernel_regularizer = regularizers.l2(0.0001),\n",
        "                      name=\"{}_conv_block\".format(name))(inputs)\n",
        "        #print('conv shape = {}'.format(conv.shape))\n",
        "        batch_norm = BatchNormalization(axis=1, name=\"{}_batch_norm\".format(name))(conv)\n",
        "        dr = Dropout(0.5)(batch_norm)\n",
        "        return Activation(\"relu\", name=\"{}_relu\".format(name))(dr) if activation else batch_norm\n",
        "    return f    \n",
        "\n",
        "\n",
        "def residual_block(block_num, **args):\n",
        "    def f(inputs):\n",
        "        res = conv_bn_relu_block(name=\"residual_1_{}\".format(block_num), activation=True, **args)(inputs)\n",
        "        res = conv_bn_relu_block(name=\"residual_2_{}\".format(block_num) , activation=False, **args)(res)\n",
        "        res = add([inputs, res], name=\"add_{}\".format(block_num))\n",
        "        return Activation(\"relu\", name=\"{}_relu\".format(block_num))(res) \n",
        "    return f\n",
        "\n",
        "\n",
        "def residual_tower(blocks, **args):\n",
        "    def f(inputs):\n",
        "        x = inputs\n",
        "        for i in range(blocks):\n",
        "            x = residual_block(block_num=i)(x)\n",
        "        return x\n",
        "    return f\n",
        "\n",
        "def convolutional_tower(blocks, **args):\n",
        "    def f(inputs):\n",
        "        x = inputs\n",
        "        for i in range(blocks):\n",
        "            x = conv_bn_relu_block(name=i)(x)\n",
        "        return x\n",
        "    return f\n",
        "\n",
        "\n",
        "def policy_head():\n",
        "    def f(inputs):\n",
        "        conv = Conv2D(filters=2, \n",
        "                      kernel_size=(3, 3),\n",
        "                      strides=(1, 1),\n",
        "                      padding=\"same\",\n",
        "                      kernel_regularizer = regularizers.l2(0.0001),\n",
        "                      name=\"policy_head_conv_block\")(inputs)\n",
        "        batch_norm = BatchNormalization(axis=1, name=\"policy_head_batch_norm\")(conv)\n",
        "        activation = Activation(\"relu\", name=\"policy_head_relu\")(batch_norm)\n",
        "        p = Flatten()(activation)\n",
        "        return Dense(units=19*19 +1, name=\"policy_out\", activation=\"softmax\")(p)\n",
        "    return f    \n",
        "\n",
        "\n",
        "def value_head():\n",
        "    def f(inputs):\n",
        "        conv = Conv2D(filters=1, \n",
        "                      kernel_size=(1, 1),\n",
        "                      strides=(1, 1),\n",
        "                      padding=\"same\",\n",
        "                      kernel_regularizer = regularizers.l2(0.0001),\n",
        "                      name=\"value_head_conv_block\")(inputs)\n",
        "        batch_norm = BatchNormalization(axis=1, name=\"value_head_batch_norm\")(conv)\n",
        "        activation = Activation(\"relu\", name=\"value_head_relu\")(batch_norm)\n",
        "        v = Flatten()(activation)\n",
        "        dense =  Dense(units= 256,  kernel_regularizer = regularizers.l2(0.0001), name=\"value_head_dense\", activation=\"relu\")(v)\n",
        "        return Dense(units= 1,  kernel_regularizer = regularizers.l2(0.0001), name=\"value_out\", activation=\"tanh\")(dense)\n",
        "    return f   \n",
        "\n",
        "\n",
        "from keras.optimizers import SGD\n",
        "from keras.models import load_model\n",
        "\n",
        "def learn(lr_=1e-4, dr_=0.7, sgf_dir=\"sgf/\", use_gpu=True, gpu_cnt=1, model_name=\"\"):\n",
        "    \n",
        "    learning_rate = lr_\n",
        "    \n",
        "    shape=(None, BSIZE, BSIZE, FEATURE_CNT)\n",
        "    \n",
        "    model = dual_residual_network(shape, blocks=BLOCK_CNT)\n",
        "\n",
        "    if os.path.isfile(sgf_dir + model_name):  \n",
        "        model = load_model(sgf_dir + model_name)\n",
        "        stdout_log('- Resore model : {}'.format(sgf_dir + model_name))\n",
        "    else:\n",
        "        stdout_log('- New model ')\n",
        "        model.compile(SGD(lr=lr_), loss=['categorical_crossentropy', 'mse'], metrics=['accuracy'])\n",
        "    \n",
        "    stdout_log(\"model.metrics_names = {}\".format(model.metrics_names))\n",
        "    #model.summary()\n",
        "\n",
        "    \n",
        "################################ Training ######################################    \n",
        "    stdout_log(\"----------- start training-------------\")\n",
        "\n",
        "    limit_time = time.time() + STEP_TIME - SAVE_TIME\n",
        "    endt = time.ctime(limit_time + JST)\n",
        "    cnvt = time.strptime(endt)\n",
        "    \n",
        "    stdout_log(\"STEP_TIME = {}\".format(STEP_TIME))\n",
        "    stdout_log(\"SAVE_TIME = {}\".format(SAVE_TIME))\n",
        "    stdout_log(\"END_TIME = {}\".format(time.strftime(\"%Y/%m/%d %H:%M:%S\", cnvt)))\n",
        "    \n",
        "    trained_txt = sgf_dir +  model_name + '_' + 'trained.txt'\n",
        "    if os.path.isfile(trained_txt) != True:\n",
        "        with open(sgf_dir +  model_name + '_' + 'trained.txt', mode='w') as f:\n",
        "            trained = [str(0), str(0), str(0), str(0), str(0.0)]\n",
        "            f.write('\\n'.join(trained)) \n",
        "        with open(sgf_dir + model_name  + '_' + 'epoch_count.txt', mode='w') as f:\n",
        "            t = str(0)\n",
        "            f.write(t)    \n",
        "    \n",
        "    #前回実行時の状態を復元\n",
        "    with open(sgf_dir + model_name + '_' + 'trained.txt') as f:\n",
        "        l_strip = [s.strip() for s in f.readlines()]\n",
        "        start_epoch_idx = int(l_strip[0])\n",
        "        start_step_idx = int(l_strip[1])\n",
        "        global_step_idx = int(l_strip[2])\n",
        "        no = int(l_strip[3])\n",
        "        max_policy = float(l_strip[4])\n",
        "\n",
        "        stdout_log(\"start_epoch_idx = {}\".format(start_epoch_idx))\n",
        "        stdout_log(\"start_step_idx = {}\".format(start_step_idx))\n",
        "        stdout_log(\"global_step_idx = {}\".format(global_step_idx))\n",
        "        stdout_log(\"feed_file_no = {}\".format(no))\n",
        "        stdout_log(\"max_policy = {}\".format(max_policy))\n",
        "        \n",
        "    with open(sgf_dir + model_name + '_' + 'epoch_count.txt') as f:\n",
        "        epoch_count = int(f.read())        \n",
        "        stdout_log(\"epoch_count = {}\".format(epoch_count))\n",
        "\n",
        "    stdout_log(\"Converting ...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    with open(sgf_dir + 'feed_n_17979696.pickle', mode='rb') as f:\n",
        "        feed_n = pickle.load(f)\n",
        "        stdout_log(\"loaded feed_n count = {}\".format(len(feed_n)))\n",
        "        stdout_log(\"loaded feed_n = {}\".format(feed_n))\n",
        "\n",
        "    # 全feed数\n",
        "    feed_cnt = 0\n",
        "\n",
        "    # 読み込みファイルNoの設定)(0～283)\n",
        "    n = 283\n",
        "    stdout_log(\"feed_files No. = {}\".format(n))\n",
        "    \n",
        "    feed_cnt = feed_n[n]\n",
        "    \n",
        "    #読み込むファイル番号を保持するリスト\n",
        "    feed_files = list(range(n + 1))\n",
        "\n",
        "    #pickleファイルのfeed数\n",
        "    feed_n_1 = []\n",
        "    for i in range(len(feed_n)):\n",
        "        if i == 0:\n",
        "            feed_n_1.append(feed_n[0])\n",
        "            continue\n",
        "        else:\n",
        "            feed_n_1.append(feed_n[i] - feed_n[i - 1])\n",
        "\n",
        "    stdout_log(\"loaded feed_n_1 count = {}\".format(len(feed_n_1)))\n",
        "    stdout_log(\"loaded feed_n_1 = {}\".format(feed_n_1))\n",
        "\n",
        "    # learning settings\n",
        "    batch_cnt =300\n",
        "    #batch_cnt = 000\n",
        "    #total_epochs = 8 * 5\n",
        "    total_epochs = 10\n",
        "    epoch_steps = feed_cnt // (batch_cnt * gpu_cnt)\n",
        "    total_steps = total_epochs * epoch_steps\n",
        "    #global_step_idx = 0\n",
        "\n",
        "\n",
        "    stdout_log(\"feed_cnt = %d\" % (feed_cnt))\n",
        "    stdout_log(\"batch_cnt = %d\" % (batch_cnt))\n",
        "    stdout_log(\"total_epochs = %d\" % (total_epochs))\n",
        "    stdout_log(\"epoch_steps = %d\" % (epoch_steps))\n",
        "    stdout_log(\"total_steps = %d\" % (total_steps))\n",
        "    stdout_log(\"learning rate = %.1g\" % (learning_rate))\n",
        "    stdout_log(\"model_name = %s\" % (model_name))\n",
        "    \n",
        "    # training\n",
        "    for epoch_idx in range(start_epoch_idx, total_epochs):\n",
        "        #if epoch_idx > 0 and (epoch_idx - 8) % 8 == 0:\n",
        "        #    learning_rate *= 0.5\n",
        "        #    stdout_log(\"learning rate=%.1g\" % (learning_rate))\n",
        "        if epoch_idx < 10:\n",
        "            learning_rate = lr_ * (1.0 - epoch_idx * 0.1)\n",
        "        else:\n",
        "            learning_rate = lr_ *  0.05\n",
        "        stdout_log(\"epoch learning rate=%.1g\" % (learning_rate))\n",
        "\n",
        "        \n",
        "        #シャッフルした後のfeedの読み込み順の累積\n",
        "        feed_n_shuffled = []\n",
        "        \n",
        "        #新しいepochに入った場合はシャッフルする\n",
        "        if epoch_idx > epoch_count - 1:\n",
        "            stdout_log(\"shuffling...\")\n",
        "            epoch_count = epoch_idx + 1\n",
        "\n",
        "            #読み込みファイルNo.の順番(ランダム)\n",
        "            random.shuffle(feed_files)\n",
        "            stdout_log(\"feed_files_shuffled = {}\".format(feed_files))\n",
        "\n",
        "            for i in range(len(feed_files)):\n",
        "                if i == 0:\n",
        "                    feed_n_shuffled.append(feed_n_1[feed_files[i]])\n",
        "                    continue\n",
        "                else:\n",
        "                    feed_n_shuffled.append(feed_n_1[feed_files[i]] + feed_n_shuffled[i - 1])\n",
        "\n",
        "            a = []\n",
        "            for i in range(len(feed_files)):\n",
        "                a.append(feed_n_1[feed_files[i]])\n",
        "\n",
        "            stdout_log(\"feed_n_1 = {}\".format(a))    \n",
        "            stdout_log(\"feed_n_shuffled = {}\".format(feed_n_shuffled))\n",
        "\n",
        "\n",
        "            with open(sgf_dir + model_name + '_' + 'epoch_count.txt', mode='w') as f:\n",
        "                f.write(str(epoch_count)) \n",
        "            with open(sgf_dir + model_name + '_' + 'feed_files.txt', mode='w') as f:\n",
        "                a_str = [str(num) for num in feed_files]\n",
        "                f.write('\\n'.join(a_str)) \n",
        "            with open(sgf_dir + model_name + '_' + 'feed_n_shuffled.txt', mode='w') as f:\n",
        "                a_str = [str(num) for num in feed_n_shuffled]\n",
        "                f.write('\\n'.join(a_str)) \n",
        "        else:\n",
        "            with open(sgf_dir + model_name + '_' + 'feed_files.txt', mode='r') as f:\n",
        "                feed_files = [int(s.strip()) for s in f.readlines()]\n",
        "                stdout_log(\"feed_files reload = {}\".format(feed_files))\n",
        "            with open(sgf_dir + model_name + '_' + 'feed_n_shuffled.txt', mode='r') as f:\n",
        "                feed_n_shuffled = [int(s.strip()) for s in f.readlines()]\n",
        "                stdout_log(\"feed_n_shuffled reload = {}\".format(feed_n_shuffled))\n",
        "\n",
        "        \n",
        "        file_number = feed_files[no]\n",
        "        \n",
        "        f = open(sgf_dir + 'feed_train_' + str(file_number) + '.pickle','rb')\n",
        "        feed_train = pickle.load(f) \n",
        "        \n",
        "        stdout_log('training : feed_train_' + str(file_number) + '.pickle')\n",
        "        stdout_log(\"training : steps = {}\".format(feed_train[0].size // batch_cnt + 1))\n",
        "        \n",
        "        for step_idx in range(start_step_idx, epoch_steps):\n",
        "            #feed_dict_ = {}\n",
        "            #feed_dict_[lr] = learning_rate\n",
        "            for gpu_idx in range(gpu_cnt):\n",
        "                \n",
        "                if feed_n_shuffled[no]  < step_idx * batch_cnt:\n",
        "                    stdout_log('training : step_idx = {}'.format(step_idx))\n",
        "                    no += 1\n",
        "                    if no > n:\n",
        "                        no = 0\n",
        "                    file_number = feed_files[no]\n",
        "                    with open(sgf_dir + 'feed_train_' + str(file_number) + '.pickle','rb') as f:\n",
        "                        feed_train = pickle.load(f) \n",
        "                        stdout_log('training : feed_train_' + str(file_number) + '.pickle')\n",
        "                        stdout_log(\"training : steps = {}\".format(feed_train[0].size // batch_cnt + 1))\n",
        "                \n",
        "                batch = feed_train[0].next_batch(batch_cnt)\n",
        "                \n",
        "                f1 = np.array(batch[0])\n",
        "                f2 = f1.reshape([-1, BSIZE, BSIZE, FEATURE_CNT])\n",
        "                #print('f shape = {}'.format(f2.shape))\n",
        "                m = np.array(batch[1])\n",
        "                #print('m shape = {}'.format(m.shape))\n",
        "                r = np.array(batch[2])\n",
        "                #print('r shape = {}'.format(r.shape))\n",
        "\n",
        "            model.fit(f2, [m, r], batch_size=batch_cnt, epochs=1, verbose=0)\n",
        "            #model.train_on_batch(f2, [m, r])\n",
        "            #model.evaluate(f2, [m, r], batch_size=batch_cnt, verbose=1) \n",
        "            #sess.run(train_op, feed_dict = feed_dict_)\n",
        "            \n",
        "            global_step_idx += 1\n",
        "\n",
        "            if global_step_idx % 100 == 0:\n",
        "                stdout_log(\"global_steps... = {}\".format(global_step_idx))\n",
        "\n",
        "            if global_step_idx % (total_steps // 1000) == 0:\n",
        "                progress_now = float(global_step_idx) / total_steps * 100\n",
        "                str_log = \"progress: %03.2f[%%] \" % (progress_now)\n",
        "\n",
        "                elapsed_time = time.time() - start_time\n",
        "                str_log += \"%03.1f\" % (elapsed_time) + \"[sec]\"\n",
        "                stdout_log(\"%s\" % (str_log))\n",
        "                start_time = time.time()\n",
        "                \n",
        "                model.save(sgf_dir + model_name)\n",
        "                stdout_log(\"save model.\")\n",
        "\n",
        "                with open(sgf_dir + model_name + '_' + 'trained.txt', mode='w') as f:\n",
        "                    trained = [str(epoch_idx), str(step_idx + 1), str(global_step_idx), str(no), str(max_policy)]\n",
        "                    f.write('\\n'.join(trained))    \n",
        "                    stdout_log(\"epoch_idx = {}, step_idx = {}, gloal_step_idx = {}, feed_file_no = {}\".format(epoch_idx, step_idx + 1, global_step_idx, no))\n",
        "            \n",
        "##################################### Test #####################################\n",
        "            # modelのテストを実施する\n",
        "            if time.time() > limit_time or (global_step_idx % (total_steps // 100) == 0):\n",
        "\n",
        "                with open(sgf_dir + 'feed_test.pickle', mode='rb') as f:\n",
        "                    feed_test = pickle.load(f)\n",
        "                    feed = [feed_train, feed_test]\n",
        "                    stdout_log(\"loaded feed_test.\")\n",
        "\n",
        "                str_log = \"\"\n",
        "                \n",
        "                test_steps = feed[1][0].size // batch_cnt\n",
        "                #np.random.shuffle(feed[0][0]._perm) \n",
        "                test_sum = [0.0, 0.0, 0.0, 0.0, 0.0]\n",
        "\n",
        "                for _ in range(test_steps):\n",
        "                    test_mini_batch = feed[1][0].next_batch(batch_cnt)\n",
        "\n",
        "                    f1 = np.array(test_mini_batch[0])\n",
        "                    f2 = f1.reshape([-1, BSIZE, BSIZE, FEATURE_CNT])\n",
        "                    m = np.array(test_mini_batch[1])\n",
        "                    r = np.array(test_mini_batch[2])\n",
        "\n",
        "                    #accur = sess.run(accuracy, feed_dict={f_acc: acc_batch[0], m_acc: acc_batch[1], r_acc: acc_batch[2]})\n",
        "                    loss, p_loss, v_loss, p_acc, v_acc = model.evaluate(f2, [m, r], verbose=0)\n",
        "\n",
        "                    #stdout_log('test : loss={} p_loss={} v_loss={} p_acc ={} v_acc={}'.format(loss, p_loss, v_loss, p_acc, v_acc))\n",
        "\n",
        "                    test_sum[0] += p_loss\n",
        "                    test_sum[1] += v_loss\n",
        "                    test_sum[2] += p_acc \n",
        "                    test_sum[3] += v_acc\n",
        "                    test_sum[4] += loss\n",
        "\n",
        "                str_log = \"loss=%3.5f p_loss=%3.5f v_loss=%3.5f p_acc=%3.5f[%%] v_acc=%3.5f[%%]\" % (test_sum[4]/test_steps, test_sum[0]/test_steps, test_sum[1]/test_steps, test_sum[2]/test_steps*100, test_sum[3]/test_steps*100)\n",
        "                stdout_log('test : {}'.format(str_log))\n",
        "                \n",
        "                \n",
        "                # テストでPolicyがMAXの場合はセーブする。（以前のものは削除する）\n",
        "                #if i == 1:\n",
        "                #    if max_policy < acc_sum[0] / acc_steps:\n",
        "                #        stdout_log('● MAX ●')\n",
        "                #        old_max_policy = max_policy\n",
        "                #        max_policy = acc_sum[0] / acc_steps\n",
        "                #\n",
        "                #        #model.save(sgf_dir + model_name + '_' + str(max_policy + '.h5'))\n",
        "                #        model.save(sgf_dir + model_name)\n",
        "\n",
        "                #        with open(sgf_dir + model_name + '_' + 'trained.txt', mode='w') as f:\n",
        "                #            trained = [str(epoch_idx), str(step_idx + 1), str(global_step_idx), str(no), str(max_policy), str(acc_sum[1]/acc_steps/2)]\n",
        "                #            f.write('\\n'.join(trained)) \n",
        "\n",
        "                        #old_filename = sgf_dir + model_name + '_' + str(old_max_policy + '.h5')\n",
        "                        #stdout_log('old_filename = {}'.format(old_filename))\n",
        "\n",
        "                        #if os.path.isfile(old_filename):\n",
        "                        #    os.remove(old_filename)\n",
        "                        #if os.path.isfile(old_filename + '.index'):\n",
        "                        #    os.remove(old_filename + '.index')\n",
        "                        #if os.path.isfile(old_filename + '.data-00000-of-00001'):\n",
        "                        #    os.remove(old_filename + '.data-00000-of-00001') \n",
        "                                \n",
        "                #stdout_log(\"%s\" % (str_log))\n",
        "                stdout_log(\"save model.\")\n",
        "                \n",
        "                model.save(sgf_dir + model_name)\n",
        "                \n",
        "                with open(sgf_dir + model_name + '.txt', mode='w') as f:\n",
        "                    now_dt = datetime.now()\n",
        "                    dt_jst = now_dt + timedelta(minutes=540)\n",
        "                    str_datetime = datetime.strftime(dt_jst, \"%Y/%m/%d-%H:%M:%S\")\n",
        "                    f.write(str_datetime + ' ' +  model_name + ' ' + str_log)\n",
        "                \n",
        "                if time.time() > limit_time:\n",
        "                    stdout_log(\"★★★ Time Limit....\")\n",
        "                    return\n",
        "            \n",
        "        #１epoch終了したので、ファイルNoを0にする。\n",
        "        no = 0\n",
        "        start_step_idx = 0\n",
        "        \n",
        "    model.save(sgf_dir + model_name)\n",
        "\n",
        "        \n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/')\n",
        "\n",
        "from learn import Feed\n",
        "\n",
        "\n",
        "STEP_TIME = 3600 * 12\n",
        "SAVE_TIME = 60 * 26\n",
        "JST = 3600 * 9\n",
        "\n",
        "BLOCK_CNT = 12\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    model = \"zero_256_\" + str(BLOCK_CNT) + \".h5\"\n",
        "    learn(1e-2, 0.5, sgf_dir=r\"/content/drive/My Drive/\", use_gpu=True, gpu_cnt=1, model_name=model)\n",
        "    #learn(3e-4, 0.5, sgf_dir=r\"/content/drive/My Drive/\", use_gpu=True, gpu_cnt=1, model_name=model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Using TensorFlow backend.\n",
            "WARNING: Logging before flag parsing goes to stderr.\n",
            "W0614 00:56:08.908958 139659460233088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:68: The name tf.get_default_graph is deprecated. Please use tf.compat.v1.get_default_graph instead.\n",
            "\n",
            "W0614 00:56:08.923959 139659460233088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:508: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
            "\n",
            "W0614 00:56:08.928651 139659460233088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3884: The name tf.truncated_normal is deprecated. Please use tf.random.truncated_normal instead.\n",
            "\n",
            "W0614 00:56:08.958652 139659460233088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:168: The name tf.get_default_session is deprecated. Please use tf.compat.v1.get_default_session instead.\n",
            "\n",
            "W0614 00:56:08.959846 139659460233088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:175: The name tf.ConfigProto is deprecated. Please use tf.compat.v1.ConfigProto instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "2019/06/14-09:56:08 - Model : dual_resnet input_shape = (None, 19, 19, 17)\n",
            "2019/06/14-09:56:08 - Model : blocks = 12\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0614 00:56:09.755563 139659460233088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:1801: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
            "\n",
            "W0614 00:56:09.833599 139659460233088 deprecation.py:506] From /usr/local/lib/python3.6/dist-packages/keras/backend/tensorflow_backend.py:3144: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "W0614 00:56:20.919645 139659460233088 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/optimizers.py:757: The name tf.train.Optimizer is deprecated. Please use tf.compat.v1.train.Optimizer instead.\n",
            "\n",
            "W0614 00:56:21.486003 139659460233088 deprecation.py:323] From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_grad.py:1250: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.where in 2.0, which has the same broadcast rule as np.where\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "2019/06/14-09:56:27 - Resore model : /content/drive/My Drive/zero_256_12.h5\n",
            "2019/06/14-09:56:27 model.metrics_names = ['loss', 'policy_out_loss', 'value_out_loss', 'policy_out_acc', 'value_out_acc']\n",
            "2019/06/14-09:56:27 ----------- start training-------------\n",
            "2019/06/14-09:56:27 STEP_TIME = 43200\n",
            "2019/06/14-09:56:27 SAVE_TIME = 1560\n",
            "2019/06/14-09:56:27 END_TIME = 2019/06/14 21:30:27\n",
            "2019/06/14-09:56:27 start_epoch_idx = 0\n",
            "2019/06/14-09:56:27 start_step_idx = 46722\n",
            "2019/06/14-09:56:27 global_step_idx = 46722\n",
            "2019/06/14-09:56:27 feed_file_no = 221\n",
            "2019/06/14-09:56:27 max_policy = 0.0\n",
            "2019/06/14-09:56:28 epoch_count = 1\n",
            "2019/06/14-09:56:28 Converting ...\n",
            "2019/06/14-09:56:28 loaded feed_n count = 284\n",
            "2019/06/14-09:56:28 loaded feed_n = [68940, 140162, 208293, 279013, 348248, 417764, 486114, 555979, 626402, 696984, 765989, 835016, 904985, 973293, 1043663, 1112155, 1181182, 1250203, 1321028, 1391217, 1461878, 1529255, 1598316, 1669832, 1738130, 1806060, 1875742, 1949450, 2019880, 2094046, 2161714, 2234320, 2302378, 2370111, 2439838, 2508882, 2576728, 2646245, 2717012, 2784860, 2854565, 2923449, 2992397, 3054822, 3118521, 3180247, 3240828, 3301534, 3365778, 3425928, 3489024, 3552528, 3616995, 3682815, 3747375, 3812032, 3870186, 3931554, 3989996, 4049917, 4113750, 4176684, 4241875, 4306412, 4370725, 4433711, 4496363, 4562894, 4626253, 4687553, 4743856, 4805300, 4865427, 4928906, 4990493, 5051893, 5112441, 5174701, 5239972, 5302861, 5366799, 5429273, 5491025, 5554221, 5619468, 5685620, 5746792, 5804722, 5863622, 5925998, 5984562, 6045089, 6108353, 6163608, 6211741, 6274803, 6336993, 6402328, 6464976, 6525395, 6585135, 6642591, 6705951, 6769166, 6826975, 6881821, 6940943, 7003517, 7063543, 7124745, 7187455, 7247333, 7304810, 7364977, 7426430, 7479865, 7544769, 7606889, 7670711, 7734905, 7793230, 7855137, 7920603, 7983162, 8046217, 8111027, 8174515, 8229454, 8289027, 8352808, 8415891, 8480601, 8545037, 8609470, 8666580, 8730017, 8791869, 8853385, 8912433, 8971722, 9034044, 9097492, 9156030, 9219175, 9282010, 9345611, 9408067, 9473131, 9536687, 9598994, 9660904, 9723486, 9789299, 9853632, 9917713, 9980600, 10044448, 10109103, 10172398, 10235217, 10297193, 10360282, 10421769, 10483809, 10546997, 10607611, 10671740, 10733823, 10795062, 10847450, 10909757, 10970350, 11033328, 11092625, 11162806, 11226163, 11286279, 11349702, 11411660, 11474829, 11539862, 11603156, 11669510, 11737754, 11796678, 11857717, 11920624, 11980214, 12044321, 12108504, 12173906, 12237703, 12301216, 12365099, 12430495, 12497445, 12562728, 12627858, 12691939, 12754734, 12816629, 12875683, 12939298, 13004301, 13059365, 13116762, 13183861, 13251935, 13316941, 13378531, 13441914, 13504593, 13561988, 13625216, 13688548, 13750566, 13810883, 13872928, 13931358, 13991278, 14050055, 14109736, 14172109, 14231967, 14293045, 14351391, 14405464, 14468208, 14533435, 14596173, 14653718, 14716675, 14775722, 14841165, 14906313, 14970889, 15032569, 15095854, 15160261, 15225984, 15290320, 15351996, 15409646, 15465410, 15532156, 15595794, 15655088, 15717953, 15780612, 15837124, 15893160, 15955196, 16021798, 16088672, 16151907, 16218425, 16280340, 16345828, 16409330, 16465152, 16529299, 16594297, 16658012, 16722948, 16786718, 16849903, 16912091, 16978550, 17042728, 17107075, 17168574, 17229457, 17292616, 17357024, 17418076, 17478971, 17538659, 17599715, 17656198, 17720275, 17783689, 17846911, 17905734, 17979696]\n",
            "2019/06/14-09:56:28 feed_files No. = 283\n",
            "2019/06/14-09:56:28 loaded feed_n_1 count = 284\n",
            "2019/06/14-09:56:28 loaded feed_n_1 = [68940, 71222, 68131, 70720, 69235, 69516, 68350, 69865, 70423, 70582, 69005, 69027, 69969, 68308, 70370, 68492, 69027, 69021, 70825, 70189, 70661, 67377, 69061, 71516, 68298, 67930, 69682, 73708, 70430, 74166, 67668, 72606, 68058, 67733, 69727, 69044, 67846, 69517, 70767, 67848, 69705, 68884, 68948, 62425, 63699, 61726, 60581, 60706, 64244, 60150, 63096, 63504, 64467, 65820, 64560, 64657, 58154, 61368, 58442, 59921, 63833, 62934, 65191, 64537, 64313, 62986, 62652, 66531, 63359, 61300, 56303, 61444, 60127, 63479, 61587, 61400, 60548, 62260, 65271, 62889, 63938, 62474, 61752, 63196, 65247, 66152, 61172, 57930, 58900, 62376, 58564, 60527, 63264, 55255, 48133, 63062, 62190, 65335, 62648, 60419, 59740, 57456, 63360, 63215, 57809, 54846, 59122, 62574, 60026, 61202, 62710, 59878, 57477, 60167, 61453, 53435, 64904, 62120, 63822, 64194, 58325, 61907, 65466, 62559, 63055, 64810, 63488, 54939, 59573, 63781, 63083, 64710, 64436, 64433, 57110, 63437, 61852, 61516, 59048, 59289, 62322, 63448, 58538, 63145, 62835, 63601, 62456, 65064, 63556, 62307, 61910, 62582, 65813, 64333, 64081, 62887, 63848, 64655, 63295, 62819, 61976, 63089, 61487, 62040, 63188, 60614, 64129, 62083, 61239, 52388, 62307, 60593, 62978, 59297, 70181, 63357, 60116, 63423, 61958, 63169, 65033, 63294, 66354, 68244, 58924, 61039, 62907, 59590, 64107, 64183, 65402, 63797, 63513, 63883, 65396, 66950, 65283, 65130, 64081, 62795, 61895, 59054, 63615, 65003, 55064, 57397, 67099, 68074, 65006, 61590, 63383, 62679, 57395, 63228, 63332, 62018, 60317, 62045, 58430, 59920, 58777, 59681, 62373, 59858, 61078, 58346, 54073, 62744, 65227, 62738, 57545, 62957, 59047, 65443, 65148, 64576, 61680, 63285, 64407, 65723, 64336, 61676, 57650, 55764, 66746, 63638, 59294, 62865, 62659, 56512, 56036, 62036, 66602, 66874, 63235, 66518, 61915, 65488, 63502, 55822, 64147, 64998, 63715, 64936, 63770, 63185, 62188, 66459, 64178, 64347, 61499, 60883, 63159, 64408, 61052, 60895, 59688, 61056, 56483, 64077, 63414, 63222, 58823, 73962]\n",
            "2019/06/14-09:56:28 feed_cnt = 17979696\n",
            "2019/06/14-09:56:28 batch_cnt = 300\n",
            "2019/06/14-09:56:28 total_epochs = 10\n",
            "2019/06/14-09:56:28 epoch_steps = 59932\n",
            "2019/06/14-09:56:28 total_steps = 599320\n",
            "2019/06/14-09:56:28 learning rate = 0.01\n",
            "2019/06/14-09:56:28 model_name = zero_256_12.h5\n",
            "2019/06/14-09:56:28 epoch learning rate=0.01\n",
            "2019/06/14-09:56:28 feed_files reload = [204, 102, 267, 26, 156, 76, 163, 157, 143, 45, 250, 131, 129, 112, 115, 255, 12, 31, 209, 244, 73, 180, 101, 29, 106, 272, 61, 221, 86, 218, 155, 256, 79, 231, 271, 222, 281, 223, 152, 24, 207, 77, 97, 219, 145, 213, 200, 264, 96, 93, 36, 121, 169, 225, 276, 142, 183, 71, 59, 166, 234, 28, 11, 132, 89, 237, 258, 176, 205, 134, 216, 162, 126, 232, 122, 117, 139, 161, 18, 172, 151, 201, 164, 32, 51, 241, 184, 194, 68, 104, 195, 48, 125, 208, 202, 2, 4, 260, 190, 211, 263, 173, 146, 14, 60, 111, 70, 23, 136, 253, 116, 279, 235, 243, 199, 140, 127, 119, 192, 181, 1, 168, 120, 67, 189, 278, 130, 43, 55, 88, 171, 22, 85, 188, 177, 6, 37, 268, 65, 47, 0, 107, 217, 203, 252, 75, 236, 53, 52, 20, 64, 240, 249, 215, 34, 13, 266, 49, 214, 50, 81, 114, 128, 193, 227, 259, 19, 108, 39, 113, 100, 158, 228, 38, 62, 248, 9, 242, 141, 159, 226, 198, 80, 187, 110, 229, 182, 167, 17, 5, 251, 30, 277, 196, 269, 84, 66, 197, 191, 144, 262, 206, 270, 179, 133, 33, 149, 69, 54, 74, 95, 150, 87, 210, 25, 98, 186, 233, 280, 90, 82, 137, 105, 212, 10, 3, 261, 154, 72, 165, 247, 124, 178, 275, 63, 283, 238, 57, 91, 123, 58, 16, 274, 257, 148, 185, 8, 273, 41, 265, 109, 7, 220, 174, 46, 40, 153, 99, 160, 27, 94, 35, 245, 103, 246, 15, 78, 56, 21, 135, 138, 92, 147, 224, 239, 254, 230, 42, 44, 170, 118, 83, 282, 175]\n",
            "2019/06/14-09:56:29 feed_n_shuffled reload = [55064, 118424, 184883, 254565, 318413, 378961, 441001, 505656, 568801, 630527, 686563, 751273, 815054, 872531, 925966, 992484, 1062453, 1135059, 1196649, 1263395, 1326874, 1391907, 1449363, 1523529, 1582651, 1645810, 1708744, 1768425, 1829597, 1888027, 1950914, 2012829, 2075718, 2138675, 2199558, 2261931, 2325153, 2385011, 2450824, 2519122, 2587196, 2649456, 2714791, 2774711, 2838312, 2901540, 2963435, 3027205, 3089395, 3144650, 3212496, 3274403, 3326791, 3385137, 3444825, 3503363, 3571607, 3633051, 3692972, 3757101, 3822249, 3892679, 3961706, 4026142, 4088518, 4151803, 4215305, 4275421, 4332818, 4389928, 4450245, 4511732, 4575220, 4634267, 4699733, 4761853, 4821142, 4884231, 4955056, 5018034, 5080616, 5139670, 5202858, 5270916, 5334420, 5396096, 5455020, 5520416, 5583775, 5641584, 5708534, 5772778, 5837588, 5902594, 5966209, 6034340, 6103575, 6167722, 6233124, 6295803, 6360739, 6420036, 6482492, 6552862, 6616695, 6676573, 6732876, 6804392, 6866244, 6933118, 6998022, 7062099, 7126675, 7182439, 7245234, 7307556, 7362495, 7426689, 7490202, 7553496, 7624718, 7685957, 7744282, 7810813, 7874996, 7931479, 7994562, 8056987, 8121644, 8180544, 8241137, 8310198, 8376350, 8440457, 8503880, 8572230, 8641747, 8705925, 8768911, 8829617, 8898557, 8961131, 9023176, 9088179, 9154781, 9216181, 9277861, 9343681, 9408148, 9478809, 9543122, 9607458, 9663970, 9725988, 9795715, 9864023, 9926211, 9986361, 10049693, 10112789, 10175263, 10236716, 10296289, 10360172, 10422916, 10478738, 10548927, 10608953, 10676801, 10736968, 10796708, 10860003, 10925230, 10995997, 11061188, 11123847, 11194429, 11252079, 11315527, 11378346, 11432419, 11496500, 11560438, 11620028, 11682738, 11745476, 11811830, 11873913, 11942934, 12012450, 12074486, 12142154, 12203210, 12268493, 12332840, 12398087, 12460739, 12525869, 12589666, 12652501, 12716216, 12783315, 12844814, 12907983, 12972416, 13040149, 13102456, 13163756, 13228316, 13289903, 13352965, 13414875, 13472805, 13536188, 13604118, 13666766, 13729673, 13795116, 13858530, 13917094, 13978846, 14040362, 14095208, 14152603, 14221608, 14292328, 14357326, 14421407, 14481534, 14542148, 14605013, 14668068, 14730026, 14790921, 14855458, 14929420, 14993827, 15055195, 15115722, 15178281, 15236723, 15305750, 15366802, 15432290, 15495846, 15556885, 15627308, 15691716, 15760600, 15823785, 15884987, 15954852, 16013629, 16083810, 16144391, 16214096, 16278429, 16338848, 16400824, 16474532, 16522665, 16591709, 16655347, 16718562, 16777856, 16846348, 16911619, 16969773, 17037150, 17100587, 17159635, 17222899, 17287963, 17349041, 17414764, 17477999, 17535544, 17604492, 17668191, 17730498, 17794320, 17857516, 17916339, 17979696]\n",
            "2019/06/14-09:56:32 training : feed_train_137.pickle\n",
            "2019/06/14-09:56:32 training : steps = 206\n",
            "2019/06/14-09:58:40 global_steps... = 46800\n",
            "2019/06/14-09:58:44 training : step_idx = 46802\n",
            "2019/06/14-09:58:47 training : feed_train_105.pickle\n",
            "2019/06/14-09:58:47 training : steps = 183\n",
            "2019/06/14-10:01:22 global_steps... = 46900\n",
            "2019/06/14-10:03:41 training : step_idx = 46985\n",
            "2019/06/14-10:03:44 training : feed_train_212.pickle\n",
            "2019/06/14-10:03:44 training : steps = 192\n",
            "2019/06/14-10:04:08 global_steps... = 47000\n",
            "2019/06/14-10:06:50 global_steps... = 47100\n",
            "2019/06/14-10:08:53 training : step_idx = 47176\n",
            "2019/06/14-10:09:00 training : feed_train_10.pickle\n",
            "2019/06/14-10:09:00 training : steps = 231\n",
            "2019/06/14-10:09:39 global_steps... = 47200\n",
            "2019/06/14-10:12:21 global_steps... = 47300\n",
            "2019/06/14-10:12:55 progress: 7.90[%] 986.9[sec]\n",
            "2019/06/14-10:13:08 save model.\n",
            "2019/06/14-10:13:08 epoch_idx = 0, step_idx = 47321, gloal_step_idx = 47321, feed_file_no = 224\n",
            "2019/06/14-10:15:16 global_steps... = 47400\n",
            "2019/06/14-10:15:26 training : step_idx = 47406\n",
            "2019/06/14-10:15:30 training : feed_train_3.pickle\n",
            "2019/06/14-10:15:30 training : steps = 236\n",
            "2019/06/14-10:18:02 global_steps... = 47500\n",
            "2019/06/14-10:20:44 global_steps... = 47600\n",
            "2019/06/14-10:21:52 training : step_idx = 47642\n",
            "2019/06/14-10:21:56 training : feed_train_261.pickle\n",
            "2019/06/14-10:21:56 training : steps = 217\n",
            "2019/06/14-10:23:30 global_steps... = 47700\n",
            "2019/06/14-10:26:11 global_steps... = 47800\n",
            "2019/06/14-10:27:45 training : step_idx = 47858\n",
            "2019/06/14-10:27:53 training : feed_train_154.pickle\n",
            "2019/06/14-10:27:53 training : steps = 214\n",
            "2019/06/14-10:29:01 global_steps... = 47900\n",
            "2019/06/14-10:29:33 progress: 8.00[%] 998.6[sec]\n",
            "2019/06/14-10:29:35 save model.\n",
            "2019/06/14-10:29:35 epoch_idx = 0, step_idx = 47920, gloal_step_idx = 47920, feed_file_no = 227\n",
            "2019/06/14-10:30:26 loaded feed_test.\n",
            "2019/06/14-10:37:15 test : p_loss=3.60505 v_loss=1.07841 p_acc=25.51016[%] v_acc=4.56782[%]\n",
            "2019/06/14-10:37:15 save model.\n",
            "2019/06/14-10:38:50 global_steps... = 48000\n",
            "2019/06/14-10:40:47 training : step_idx = 48072\n",
            "2019/06/14-10:40:51 training : feed_train_72.pickle\n",
            "2019/06/14-10:40:51 training : steps = 201\n",
            "2019/06/14-10:41:36 global_steps... = 48100\n",
            "2019/06/14-10:44:18 global_steps... = 48200\n",
            "2019/06/14-10:46:14 training : step_idx = 48272\n",
            "2019/06/14-10:46:18 training : feed_train_165.pickle\n",
            "2019/06/14-10:46:18 training : steps = 203\n",
            "2019/06/14-10:47:04 global_steps... = 48300\n",
            "2019/06/14-10:49:46 global_steps... = 48400\n",
            "2019/06/14-10:51:46 training : step_idx = 48474\n",
            "2019/06/14-10:51:53 training : feed_train_247.pickle\n",
            "2019/06/14-10:51:53 training : steps = 210\n",
            "2019/06/14-10:52:36 global_steps... = 48500\n",
            "2019/06/14-10:53:06 progress: 8.10[%] 1413.3[sec]\n",
            "2019/06/14-10:53:10 save model.\n",
            "2019/06/14-10:53:10 epoch_idx = 0, step_idx = 48519, gloal_step_idx = 48519, feed_file_no = 230\n",
            "2019/06/14-10:55:21 global_steps... = 48600\n",
            "2019/06/14-10:57:37 training : step_idx = 48684\n",
            "2019/06/14-10:57:55 training : feed_train_124.pickle\n",
            "2019/06/14-10:57:55 training : steps = 211\n",
            "2019/06/14-10:58:21 global_steps... = 48700\n",
            "2019/06/14-11:01:04 global_steps... = 48800\n",
            "2019/06/14-11:03:36 training : step_idx = 48894\n",
            "2019/06/14-11:03:39 training : feed_train_178.pickle\n",
            "2019/06/14-11:03:39 training : steps = 207\n",
            "2019/06/14-11:03:49 global_steps... = 48900\n",
            "2019/06/14-11:06:31 global_steps... = 49000\n",
            "2019/06/14-11:09:13 global_steps... = 49100\n",
            "2019/06/14-11:09:15 training : step_idx = 49101\n",
            "2019/06/14-11:09:19 training : feed_train_275.pickle\n",
            "2019/06/14-11:09:19 training : steps = 203\n",
            "2019/06/14-11:09:47 progress: 8.20[%] 1000.4[sec]\n",
            "2019/06/14-11:09:49 save model.\n",
            "2019/06/14-11:09:49 epoch_idx = 0, step_idx = 49118, gloal_step_idx = 49118, feed_file_no = 233\n",
            "2019/06/14-11:12:02 global_steps... = 49200\n",
            "2019/06/14-11:14:44 global_steps... = 49300\n",
            "2019/06/14-11:14:52 training : step_idx = 49304\n",
            "2019/06/14-11:14:59 training : feed_train_63.pickle\n",
            "2019/06/14-11:14:59 training : steps = 216\n",
            "2019/06/14-11:17:35 global_steps... = 49400\n",
            "2019/06/14-11:20:17 global_steps... = 49500\n",
            "2019/06/14-11:20:48 training : step_idx = 49519\n",
            "2019/06/14-11:20:50 training : feed_train_283.pickle\n",
            "2019/06/14-11:20:50 training : steps = 124\n",
            "2019/06/14-11:23:02 global_steps... = 49600\n",
            "2019/06/14-11:25:43 global_steps... = 49700\n",
            "2019/06/14-11:26:10 progress: 8.30[%] 983.6[sec]\n",
            "2019/06/14-11:26:14 save model.\n",
            "2019/06/14-11:26:14 epoch_idx = 0, step_idx = 49717, gloal_step_idx = 49717, feed_file_no = 235\n",
            "2019/06/14-11:27:32 training : step_idx = 49765\n",
            "2019/06/14-11:27:36 training : feed_train_238.pickle\n",
            "2019/06/14-11:27:36 training : steps = 215\n",
            "2019/06/14-11:28:33 global_steps... = 49800\n",
            "2019/06/14-11:31:15 global_steps... = 49900\n",
            "2019/06/14-11:33:25 training : step_idx = 49980\n",
            "2019/06/14-11:33:29 training : feed_train_57.pickle\n",
            "2019/06/14-11:33:29 training : steps = 205\n",
            "2019/06/14-11:34:01 global_steps... = 50000\n",
            "2019/06/14-11:36:43 global_steps... = 50100\n",
            "2019/06/14-11:38:59 training : step_idx = 50184\n",
            "2019/06/14-11:39:03 training : feed_train_91.pickle\n",
            "2019/06/14-11:39:03 training : steps = 202\n",
            "2019/06/14-11:39:29 global_steps... = 50200\n",
            "2019/06/14-11:42:11 global_steps... = 50300\n",
            "2019/06/14-11:42:37 progress: 8.40[%] 986.8[sec]\n",
            "2019/06/14-11:42:40 save model.\n",
            "2019/06/14-11:42:40 epoch_idx = 0, step_idx = 50316, gloal_step_idx = 50316, feed_file_no = 238\n",
            "2019/06/14-11:44:34 training : step_idx = 50386\n",
            "2019/06/14-11:44:40 training : feed_train_123.pickle\n",
            "2019/06/14-11:44:40 training : steps = 209\n",
            "2019/06/14-11:45:03 global_steps... = 50400\n",
            "2019/06/14-11:47:45 global_steps... = 50500\n",
            "2019/06/14-11:50:19 training : step_idx = 50595\n",
            "2019/06/14-11:50:24 training : feed_train_58.pickle\n",
            "2019/06/14-11:50:24 training : steps = 195\n",
            "2019/06/14-11:50:32 global_steps... = 50600\n",
            "2019/06/14-11:53:14 global_steps... = 50700\n",
            "2019/06/14-11:55:41 training : step_idx = 50790\n",
            "2019/06/14-11:55:48 training : feed_train_16.pickle\n",
            "2019/06/14-11:55:48 training : steps = 231\n",
            "2019/06/14-11:56:04 global_steps... = 50800\n",
            "2019/06/14-11:58:47 global_steps... = 50900\n",
            "2019/06/14-11:59:11 progress: 8.50[%] 993.7[sec]\n",
            "2019/06/14-11:59:14 save model.\n",
            "2019/06/14-11:59:15 epoch_idx = 0, step_idx = 50915, gloal_step_idx = 50915, feed_file_no = 241\n",
            "2019/06/14-12:01:32 global_steps... = 51000\n",
            "2019/06/14-12:02:05 training : step_idx = 51020\n",
            "2019/06/14-12:02:13 training : feed_train_274.pickle\n",
            "2019/06/14-12:02:13 training : steps = 204\n",
            "2019/06/14-12:04:23 global_steps... = 51100\n",
            "2019/06/14-12:07:05 global_steps... = 51200\n",
            "2019/06/14-12:07:42 training : step_idx = 51223\n",
            "2019/06/14-12:07:50 training : feed_train_257.pickle\n",
            "2019/06/14-12:07:50 training : steps = 219\n",
            "2019/06/14-12:09:55 global_steps... = 51300\n",
            "2019/06/14-12:12:37 global_steps... = 51400\n",
            "2019/06/14-12:13:43 training : step_idx = 51441\n",
            "2019/06/14-12:13:49 training : feed_train_148.pickle\n",
            "2019/06/14-12:13:49 training : steps = 212\n",
            "2019/06/14-12:15:25 global_steps... = 51500\n",
            "2019/06/14-12:15:48 progress: 8.60[%] 997.0[sec]\n",
            "2019/06/14-12:15:50 save model.\n",
            "2019/06/14-12:15:50 epoch_idx = 0, step_idx = 51514, gloal_step_idx = 51514, feed_file_no = 244\n",
            "2019/06/14-12:18:10 global_steps... = 51600\n",
            "2019/06/14-12:19:37 training : step_idx = 51653\n",
            "2019/06/14-12:19:42 training : feed_train_185.pickle\n",
            "2019/06/14-12:19:42 training : steps = 204\n",
            "2019/06/14-12:20:58 global_steps... = 51700\n",
            "2019/06/14-12:23:40 global_steps... = 51800\n",
            "2019/06/14-12:25:12 training : step_idx = 51857\n",
            "2019/06/14-12:25:22 training : feed_train_8.pickle\n",
            "2019/06/14-12:25:22 training : steps = 235\n",
            "2019/06/14-12:26:32 global_steps... = 51900\n",
            "2019/06/14-12:29:14 global_steps... = 52000\n",
            "2019/06/14-12:31:43 training : step_idx = 52092\n",
            "2019/06/14-12:31:49 training : feed_train_273.pickle\n",
            "2019/06/14-12:31:49 training : steps = 215\n",
            "2019/06/14-12:32:02 global_steps... = 52100\n",
            "2019/06/14-12:32:23 progress: 8.70[%] 995.2[sec]\n",
            "2019/06/14-12:32:26 save model.\n",
            "2019/06/14-12:32:26 epoch_idx = 0, step_idx = 52113, gloal_step_idx = 52113, feed_file_no = 247\n",
            "2019/06/14-12:34:47 global_steps... = 52200\n",
            "2019/06/14-12:37:29 global_steps... = 52300\n",
            "2019/06/14-12:37:38 training : step_idx = 52306\n",
            "2019/06/14-12:37:44 training : feed_train_41.pickle\n",
            "2019/06/14-12:37:44 training : steps = 230\n",
            "2019/06/14-12:40:17 global_steps... = 52400\n",
            "2019/06/14-12:42:59 global_steps... = 52500\n",
            "2019/06/14-12:43:57 training : step_idx = 52536\n",
            "2019/06/14-12:44:03 training : feed_train_265.pickle\n",
            "2019/06/14-12:44:03 training : steps = 211\n",
            "2019/06/14-12:45:47 global_steps... = 52600\n",
            "2019/06/14-12:48:29 global_steps... = 52700\n",
            "2019/06/14-12:48:49 progress: 8.80[%] 985.5[sec]\n",
            "2019/06/14-12:48:51 save model.\n",
            "2019/06/14-12:48:51 epoch_idx = 0, step_idx = 52712, gloal_step_idx = 52712, feed_file_no = 249\n",
            "2019/06/14-12:49:46 training : step_idx = 52746\n",
            "2019/06/14-12:49:54 training : feed_train_109.pickle\n",
            "2019/06/14-12:49:54 training : steps = 205\n",
            "2019/06/14-12:51:22 global_steps... = 52800\n",
            "2019/06/14-12:54:04 global_steps... = 52900\n",
            "2019/06/14-12:55:25 training : step_idx = 52950\n",
            "2019/06/14-12:55:31 training : feed_train_7.pickle\n",
            "2019/06/14-12:55:31 training : steps = 233\n",
            "2019/06/14-12:56:52 global_steps... = 53000\n",
            "2019/06/14-12:59:34 global_steps... = 53100\n",
            "2019/06/14-13:01:49 training : step_idx = 53183\n",
            "2019/06/14-13:01:55 training : feed_train_220.pickle\n",
            "2019/06/14-13:01:55 training : steps = 196\n",
            "2019/06/14-13:02:23 global_steps... = 53200\n",
            "2019/06/14-13:05:04 global_steps... = 53300\n",
            "2019/06/14-13:05:22 progress: 8.90[%] 993.4[sec]\n",
            "2019/06/14-13:05:25 save model.\n",
            "2019/06/14-13:05:25 epoch_idx = 0, step_idx = 53311, gloal_step_idx = 53311, feed_file_no = 252\n",
            "2019/06/14-13:07:17 training : step_idx = 53379\n",
            "2019/06/14-13:07:26 training : feed_train_174.pickle\n",
            "2019/06/14-13:07:26 training : steps = 234\n",
            "2019/06/14-13:08:00 global_steps... = 53400\n",
            "2019/06/14-13:10:42 global_steps... = 53500\n",
            "2019/06/14-13:13:24 global_steps... = 53600\n",
            "2019/06/14-13:13:47 training : step_idx = 53613\n",
            "2019/06/14-13:13:52 training : feed_train_46.pickle\n",
            "2019/06/14-13:13:52 training : steps = 202\n",
            "2019/06/14-13:16:13 global_steps... = 53700\n",
            "2019/06/14-13:18:55 global_steps... = 53800\n",
            "2019/06/14-13:19:19 training : step_idx = 53815\n",
            "2019/06/14-13:19:27 training : feed_train_40.pickle\n",
            "2019/06/14-13:19:27 training : steps = 233\n",
            "2019/06/14-13:21:45 global_steps... = 53900\n",
            "2019/06/14-13:22:01 progress: 9.00[%] 998.9[sec]\n",
            "2019/06/14-13:22:05 save model.\n",
            "2019/06/14-13:22:05 epoch_idx = 0, step_idx = 53910, gloal_step_idx = 53910, feed_file_no = 255\n",
            "2019/06/14-13:22:57 loaded feed_test.\n",
            "2019/06/14-13:29:46 test : p_loss=3.23986 v_loss=1.10572 p_acc=31.35310[%] v_acc=9.64525[%]\n",
            "2019/06/14-13:29:46 save model.\n",
            "2019/06/14-13:31:31 global_steps... = 54000\n",
            "2019/06/14-13:32:47 training : step_idx = 54047\n",
            "2019/06/14-13:32:55 training : feed_train_153.pickle\n",
            "2019/06/14-13:32:55 training : steps = 215\n",
            "2019/06/14-13:34:21 global_steps... = 54100\n",
            "2019/06/14-13:37:03 global_steps... = 54200\n",
            "2019/06/14-13:38:44 training : step_idx = 54262\n",
            "2019/06/14-13:38:50 training : feed_train_99.pickle\n",
            "2019/06/14-13:38:50 training : steps = 202\n",
            "2019/06/14-13:39:51 global_steps... = 54300\n",
            "2019/06/14-13:42:33 global_steps... = 54400\n",
            "2019/06/14-13:44:15 training : step_idx = 54463\n",
            "2019/06/14-13:44:21 training : feed_train_160.pickle\n",
            "2019/06/14-13:44:21 training : steps = 207\n",
            "2019/06/14-13:45:21 global_steps... = 54500\n",
            "2019/06/14-13:45:35 progress: 9.10[%] 1414.4[sec]\n",
            "2019/06/14-13:45:38 save model.\n",
            "2019/06/14-13:45:38 epoch_idx = 0, step_idx = 54509, gloal_step_idx = 54509, feed_file_no = 258\n",
            "2019/06/14-13:48:05 global_steps... = 54600\n",
            "2019/06/14-13:49:59 training : step_idx = 54670\n",
            "2019/06/14-13:50:07 training : feed_train_27.pickle\n",
            "2019/06/14-13:50:07 training : steps = 246\n",
            "2019/06/14-13:50:56 global_steps... = 54700\n",
            "2019/06/14-13:53:37 global_steps... = 54800\n",
            "2019/06/14-13:56:19 global_steps... = 54900\n",
            "2019/06/14-13:56:46 training : step_idx = 54916\n",
            "2019/06/14-13:56:53 training : feed_train_94.pickle\n",
            "2019/06/14-13:56:53 training : steps = 161\n",
            "2019/06/14-13:59:09 global_steps... = 55000\n",
            "2019/06/14-14:01:12 training : step_idx = 55076\n",
            "2019/06/14-14:01:16 training : feed_train_35.pickle\n",
            "2019/06/14-14:01:16 training : steps = 231\n",
            "2019/06/14-14:01:55 global_steps... = 55100\n",
            "2019/06/14-14:02:08 progress: 9.20[%] 993.1[sec]\n",
            "2019/06/14-14:02:11 save model.\n",
            "2019/06/14-14:02:11 epoch_idx = 0, step_idx = 55108, gloal_step_idx = 55108, feed_file_no = 261\n",
            "2019/06/14-14:04:39 global_steps... = 55200\n",
            "2019/06/14-14:07:21 global_steps... = 55300\n",
            "2019/06/14-14:07:31 training : step_idx = 55306\n",
            "2019/06/14-14:07:36 training : feed_train_245.pickle\n",
            "2019/06/14-14:07:36 training : steps = 213\n",
            "2019/06/14-14:10:08 global_steps... = 55400\n",
            "2019/06/14-14:12:50 global_steps... = 55500\n",
            "2019/06/14-14:13:19 training : step_idx = 55518\n",
            "2019/06/14-14:13:26 training : feed_train_103.pickle\n",
            "2019/06/14-14:13:26 training : steps = 211\n",
            "2019/06/14-14:15:39 global_steps... = 55600\n",
            "2019/06/14-14:18:21 global_steps... = 55700\n",
            "2019/06/14-14:18:32 progress: 9.30[%] 983.6[sec]\n",
            "2019/06/14-14:18:35 save model.\n",
            "2019/06/14-14:18:35 epoch_idx = 0, step_idx = 55707, gloal_step_idx = 55707, feed_file_no = 263\n",
            "2019/06/14-14:19:11 training : step_idx = 55729\n",
            "2019/06/14-14:19:17 training : feed_train_246.pickle\n",
            "2019/06/14-14:19:17 training : steps = 198\n",
            "2019/06/14-14:21:12 global_steps... = 55800\n",
            "2019/06/14-14:23:54 global_steps... = 55900\n",
            "2019/06/14-14:24:38 training : step_idx = 55927\n",
            "2019/06/14-14:24:45 training : feed_train_15.pickle\n",
            "2019/06/14-14:24:45 training : steps = 229\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwJKx2Zk8zmr",
        "colab_type": "code",
        "outputId": "08c53958-8951-4002-8344-f70f7c5fb9a5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 343
        }
      },
      "source": [
        "pip install keras==2.1.6"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting keras==2.1.6\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/54/e8/eaff7a09349ae9bd40d3ebaf028b49f5e2392c771f294910f75bb608b241/Keras-2.1.6-py2.py3-none-any.whl (339kB)\n",
            "\u001b[K     |████████████████████████████████| 348kB 47.1MB/s \n",
            "\u001b[?25hRequirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (2.8.0)\n",
            "Requirement already satisfied: scipy>=0.14 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.3.0)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (3.13)\n",
            "Requirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.12.0)\n",
            "Requirement already satisfied: numpy>=1.9.1 in /usr/local/lib/python3.6/dist-packages (from keras==2.1.6) (1.16.4)\n",
            "Installing collected packages: keras\n",
            "  Found existing installation: Keras 2.2.4\n",
            "    Uninstalling Keras-2.2.4:\n",
            "      Successfully uninstalled Keras-2.2.4\n",
            "Successfully installed keras-2.1.6\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "keras"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    }
  ]
}