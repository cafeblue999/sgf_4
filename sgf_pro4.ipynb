{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "sgf_pro4.ipynb ",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cafeblue999/sgf_4/blob/master/sgf_pro4.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "4mb6n7hL2nWF",
        "colab_type": "code",
        "outputId": "361f78ec-a6c3-4505-8a3d-013f4ce37644",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 11055
        }
      },
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "# -*- coding: utf-8 -*-\n",
        "\n",
        "from sys import stderr, stdout, exit\n",
        "from datetime import datetime, date, timedelta\n",
        "import numpy as np\n",
        "import logging\n",
        "import google.colab\n",
        "import googleapiclient.discovery\n",
        "import googleapiclient.http\n",
        "import pickle\n",
        "import tensorflow as tf\n",
        "import glob\n",
        "import time\n",
        "from collections import Counter\n",
        "import random\n",
        "import os\n",
        "\n",
        "from google.colab import files\n",
        "from google.colab import drive\n",
        "\n",
        "\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "np.set_printoptions(threshold=np.inf)\n",
        "\n",
        "basename = datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "log_file = 'log_' + basename + '.txt'\n",
        "\n",
        "logging.basicConfig(format='%(asctime)s\\t%(levelname)s\\t%(message)s', datefmt='%Y/%m/%d %H:%M:%S', filename=log_file, level=logging.INFO)\n",
        "\n",
        "def stdout_log(str):\n",
        "    now_dt = datetime.now()\n",
        "    dt_jst = now_dt + timedelta(minutes=540)\n",
        "    str_datetime = datetime.strftime(dt_jst, \"%Y/%m/%d-%H:%M:%S\")\n",
        "    stdout.write(str_datetime + \" \" + str + \"\\n\")\n",
        "    #log_file = open(\"log.txt\", \"a\")\n",
        "    #log_file.write(str)\n",
        "    #log_file.close()\n",
        "\n",
        "    \n",
        "BSIZE = 19  # board size\n",
        "EBSIZE = BSIZE + 2  # extended board size\n",
        "BVCNT = BSIZE ** 2  # vertex count\n",
        "EBVCNT = EBSIZE ** 2  # extended vertex count\n",
        "PASS = EBVCNT  # pass\n",
        "VNULL = EBVCNT + 1  # invalid position\n",
        "KOMI = 6.5\n",
        "dir4 = [1, EBSIZE, -1, -EBSIZE]\n",
        "diag4 = [1 + EBSIZE, EBSIZE - 1, -EBSIZE - 1, 1 - EBSIZE]\n",
        "KEEP_PREV_CNT = 7\n",
        "FEATURE_CNT = KEEP_PREV_CNT * 2 + 3  # 7\n",
        "x_labels = \"ABCDEFGHJKLMNOPQRST\"\n",
        "\n",
        "\n",
        "def ev2xy(ev):\n",
        "    return ev % EBSIZE, ev // EBSIZE\n",
        "\n",
        "\n",
        "def xy2ev(x, y):\n",
        "    return y * EBSIZE + x\n",
        "\n",
        "\n",
        "def rv2ev(rv):\n",
        "    if rv == BVCNT:\n",
        "        return PASS\n",
        "    return rv % BSIZE + 1 + (rv // BSIZE + 1) * EBSIZE\n",
        "\n",
        "\n",
        "def ev2rv(ev):\n",
        "    if ev == PASS:\n",
        "        return BVCNT\n",
        "    return ev % EBSIZE - 1 + (ev // EBSIZE - 1) * BSIZE\n",
        "\n",
        "\n",
        "def ev2str(ev):\n",
        "    if ev >= PASS:\n",
        "        \n",
        "        return \"PASS\"\n",
        "    x, y = ev2xy(ev)\n",
        "    return x_labels[x - 1] + str(y)\n",
        "\n",
        "\n",
        "def str2ev(v_str):\n",
        "    v_str = v_str.upper()\n",
        "    if v_str == \"PASS\" or v_str == \"RESIGN\":\n",
        "        return PASS\n",
        "    else:\n",
        "        x = x_labels.find(v_str[0]) + 1\n",
        "        y = int(v_str[1:])\n",
        "        return xy2ev(x, y)\n",
        "\n",
        "def turn2str(turn):\n",
        "    if turn == 0:\n",
        "        return 'W'\n",
        "    else:\n",
        "        return 'B'\n",
        "\n",
        "\n",
        "rv_list = [rv2ev(i) for i in range(BVCNT)]\n",
        "\n",
        "\n",
        "class StoneGroup(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.lib_cnt = VNULL  # liberty count\n",
        "        self.size = VNULL  # stone size\n",
        "        self.v_atr = VNULL  # liberty position if in Atari\n",
        "        self.libs = set()  # set of liberty positions\n",
        "\n",
        "    def clear(self, stone=True):\n",
        "        # clear as placed stone or empty\n",
        "        self.lib_cnt = 0 if stone else VNULL\n",
        "        self.size = 1 if stone else VNULL\n",
        "        self.v_atr = VNULL\n",
        "        self.libs.clear()\n",
        "\n",
        "    def add(self, v):\n",
        "        # add liberty at v\n",
        "        if v not in self.libs:\n",
        "            self.libs.add(v)\n",
        "            self.lib_cnt += 1\n",
        "            self.v_atr = v\n",
        "\n",
        "    def sub(self, v):\n",
        "        # remove liberty at v\n",
        "        if v in self.libs:\n",
        "            self.libs.remove(v)\n",
        "            self.lib_cnt -= 1\n",
        "\n",
        "    def merge(self, other):\n",
        "        # merge with aother stone group\n",
        "        self.libs |= other.libs\n",
        "        self.lib_cnt = len(self.libs)\n",
        "        self.size += other.size\n",
        "        if self.lib_cnt == 1:\n",
        "            for lib in self.libs:\n",
        "                self.v_atr = lib\n",
        "\n",
        "\n",
        "class Board(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        # 1-d array ([EBVCNT]) of stones or empty or exterior\n",
        "        # 0: white 1: black\n",
        "        # 2: empty 3: exterior\n",
        "        self.color = np.full(EBVCNT, 3)\n",
        "        self.sg = [StoneGroup() for _ in range(EBVCNT)]  # stone groups\n",
        "        self.clear()\n",
        "\n",
        "    def clear(self):\n",
        "        self.color[rv_list] = 2  # empty\n",
        "        self.id = np.arange(EBVCNT)  # id of stone group\n",
        "        self.next = np.arange(EBVCNT)  # next position in the same group\n",
        "        for i in range(EBVCNT):\n",
        "            self.sg[i].clear(stone=False)\n",
        "        self.prev_color = [np.copy(self.color) for _ in range(KEEP_PREV_CNT)]\n",
        "\n",
        "        self.ko = VNULL  # illegal position due to Ko\n",
        "        self.turn = 1  # black\n",
        "        self.move_cnt = 0  # move count\n",
        "        self.prev_move = VNULL  # previous move\n",
        "        self.remove_cnt = 0  # removed stones count\n",
        "        self.history = []\n",
        "\n",
        "    def copy(self, b_cpy):\n",
        "        b_cpy.color = np.copy(self.color)\n",
        "        b_cpy.id = np.copy(self.id)\n",
        "        b_cpy.next = np.copy(self.next)\n",
        "        for i in range(EBVCNT):\n",
        "            b_cpy.sg[i].lib_cnt = self.sg[i].lib_cnt\n",
        "            b_cpy.sg[i].size = self.sg[i].size\n",
        "            b_cpy.sg[i].v_atr = self.sg[i].v_atr\n",
        "            b_cpy.sg[i].libs |= self.sg[i].libs\n",
        "        for i in range(KEEP_PREV_CNT):\n",
        "            b_cpy.prev_color[i] = np.copy(self.prev_color[i])\n",
        "\n",
        "        b_cpy.ko = self.ko\n",
        "        b_cpy.turn = self.turn\n",
        "        b_cpy.move_cnt = self.move_cnt\n",
        "        b_cpy.prev_move = self.prev_move\n",
        "        b_cpy.remove_cnt = self.remove_cnt\n",
        "\n",
        "        for h in self.history:\n",
        "            b_cpy.history.append(h)\n",
        "\n",
        "    def remove(self, v):\n",
        "        # remove stone group including stone at v\n",
        "        v_tmp = v\n",
        "        while 1:\n",
        "            self.remove_cnt += 1\n",
        "            self.color[v_tmp] = 2  # empty\n",
        "            self.id[v_tmp] = v_tmp  # reset id\n",
        "            for d in dir4:\n",
        "                nv = v_tmp + d\n",
        "                # add liberty to neighbor groups\n",
        "                self.sg[self.id[nv]].add(v_tmp)\n",
        "            v_next = self.next[v_tmp]\n",
        "            self.next[v_tmp] = v_tmp\n",
        "            v_tmp = v_next\n",
        "            if v_tmp == v:\n",
        "                break  # finish when all stones are removed\n",
        "\n",
        "    def merge(self, v1, v2):\n",
        "        # merge stone groups at v1 and v2\n",
        "        id_base = self.id[v1]\n",
        "        id_add = self.id[v2]\n",
        "        if self.sg[id_base].size < self.sg[id_add].size:\n",
        "            id_base, id_add = id_add, id_base  # swap\n",
        "        self.sg[id_base].merge(self.sg[id_add])\n",
        "\n",
        "        v_tmp = id_add\n",
        "        while 1:\n",
        "            self.id[v_tmp] = id_base  # change id to id_base\n",
        "            v_tmp = self.next[v_tmp]\n",
        "            if v_tmp == id_add:\n",
        "                break\n",
        "        # swap next id for circulation\n",
        "        self.next[v1], self.next[v2] = self.next[v2], self.next[v1]\n",
        "\n",
        "    def place_stone(self, v):\n",
        "        self.color[v] = self.turn\n",
        "        self.id[v] = v\n",
        "        self.sg[self.id[v]].clear(stone=True)\n",
        "        for d in dir4:\n",
        "            nv = v + d\n",
        "            if self.color[nv] == 2:\n",
        "                self.sg[self.id[v]].add(nv)  # add liberty\n",
        "            else:\n",
        "                self.sg[self.id[nv]].sub(v)  # remove liberty\n",
        "        # merge stone groups\n",
        "        for d in dir4:\n",
        "            nv = v + d\n",
        "            if self.color[nv] == self.turn and self.id[nv] != self.id[v]:\n",
        "                self.merge(v, nv)\n",
        "        # remove opponent's stones\n",
        "        self.remove_cnt = 0\n",
        "        for d in dir4:\n",
        "            nv = v + d\n",
        "            if self.color[nv] == int(self.turn == 0) and \\\n",
        "                    self.sg[self.id[nv]].lib_cnt == 0:\n",
        "                self.remove(nv)\n",
        "\n",
        "    def legal(self, v):\n",
        "        if v == PASS:\n",
        "            return True\n",
        "        elif v == self.ko or self.color[v] != 2:\n",
        "            return False\n",
        "\n",
        "        stone_cnt = [0, 0]\n",
        "        atr_cnt = [0, 0]\n",
        "        for d in dir4:\n",
        "            nv = v + d\n",
        "            c = self.color[nv]\n",
        "            if c == 2:\n",
        "                return True\n",
        "            elif c <= 1:\n",
        "                stone_cnt[c] += 1\n",
        "                if self.sg[self.id[nv]].lib_cnt == 1:\n",
        "                    atr_cnt[c] += 1\n",
        "\n",
        "        return (atr_cnt[int(self.turn == 0)] != 0 or\n",
        "                atr_cnt[self.turn] < stone_cnt[self.turn])\n",
        "\n",
        "    def eyeshape(self, v, pl):\n",
        "        if v == PASS:\n",
        "            return False\n",
        "        for d in dir4:\n",
        "            c = self.color[v + d]\n",
        "            if c == 2 or c == int(pl == 0):\n",
        "                return False\n",
        "\n",
        "        diag_cnt = [0, 0, 0, 0]\n",
        "        for d in diag4:\n",
        "            nv = v + d\n",
        "            diag_cnt[self.color[nv]] += 1\n",
        "\n",
        "        wedge_cnt = diag_cnt[int(pl == 0)] + int(diag_cnt[3] > 0)\n",
        "        if wedge_cnt == 2:\n",
        "            for d in diag4:\n",
        "                nv = v + d\n",
        "                if self.color[nv] == int(pl == 0) and \\\n",
        "                        self.sg[self.id[nv]].lib_cnt == 1 and \\\n",
        "                        self.sg[self.id[nv]].v_atr != self.ko:\n",
        "                    return True\n",
        "\n",
        "        return wedge_cnt < 2\n",
        "\n",
        "    def play(self, v, not_fill_eye=True):\n",
        "\n",
        "        #logging.info(\"board play : {}\".format(ev2str(v)))\n",
        "\n",
        "        if not self.legal(v):\n",
        "            logging.info(\"board play : not legal = {}\".format(ev2str(v)))\n",
        "            return 1\n",
        "        elif not_fill_eye and self.eyeshape(v, self.turn):\n",
        "            return 2\n",
        "        else:\n",
        "            for i in range(KEEP_PREV_CNT - 1)[::-1]:\n",
        "                self.prev_color[i + 1] = np.copy(self.prev_color[i])\n",
        "            self.prev_color[0] = np.copy(self.color)\n",
        "\n",
        "            if v == PASS:\n",
        "                self.ko = VNULL\n",
        "            else:\n",
        "                self.place_stone(v)\n",
        "                id = self.id[v]\n",
        "                self.ko = VNULL\n",
        "                if self.remove_cnt == 1 and \\\n",
        "                        self.sg[id].lib_cnt == 1 and \\\n",
        "                        self.sg[id].size == 1:\n",
        "                    self.ko = self.sg[id].v_atr\n",
        "\n",
        "        self.prev_move = v\n",
        "        self.history.append(v)\n",
        "        self.turn = int(self.turn == 0)\n",
        "        self.move_cnt += 1\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def random_play(self):\n",
        "        empty_list = np.where(self.color == 2)[0]\n",
        "        np.random.shuffle(empty_list)\n",
        "\n",
        "        for v in empty_list:\n",
        "            if self.play(v, True) == 0:\n",
        "                return v\n",
        "\n",
        "        self.play(PASS)\n",
        "        return PASS\n",
        "\n",
        "    def score(self):\n",
        "        stone_cnt = [0, 0]\n",
        "        for rv in range(BVCNT):\n",
        "            v = rv2ev(rv)\n",
        "            c = self.color[v]\n",
        "            if c <= 1:\n",
        "                stone_cnt[c] += 1\n",
        "            else:\n",
        "                nbr_cnt = [0, 0, 0, 0]\n",
        "                for d in dir4:\n",
        "                    nbr_cnt[self.color[v + d]] += 1\n",
        "                if nbr_cnt[0] > 0 and nbr_cnt[1] == 0:\n",
        "                    stone_cnt[0] += 1\n",
        "                elif nbr_cnt[1] > 0 and nbr_cnt[0] == 0:\n",
        "                    stone_cnt[1] += 1\n",
        "        return stone_cnt[1] - stone_cnt[0] - KOMI\n",
        "\n",
        "    def rollout(self, show_board=False):\n",
        "        while self.move_cnt < EBVCNT * 2:\n",
        "            prev_move = self.prev_move\n",
        "            move = self.random_play()\n",
        "            if show_board and move != PASS:\n",
        "                stderr.write(\"\\nmove count=%d\\n\" % b.move_cnt)\n",
        "                b.showboard()\n",
        "            if prev_move == PASS and move == PASS:\n",
        "                break\n",
        "\n",
        "    def showboard(self):\n",
        "\n",
        "        def pirnt_xlabel():\n",
        "            line_str = \"  \"\n",
        "            for x in range(BSIZE):\n",
        "                line_str += \" \" + x_labels[x] + \" \"\n",
        "            stderr.write(line_str + \"\\n\")\n",
        "\n",
        "        pirnt_xlabel()\n",
        "\n",
        "        for y in range(1, BSIZE + 1)[::-1]:  # 9, 8, ..., 1\n",
        "            line_str = str(y) if y >= 10 else \" \" + str(y)\n",
        "            for x in range(1, BSIZE + 1):\n",
        "                v = xy2ev(x, y)\n",
        "                x_str = \" . \"\n",
        "                color = self.color[v]\n",
        "                if color <= 1:\n",
        "                    stone_str = \"O\" if color == 0 else \"X\"\n",
        "                    if v == self.prev_move:\n",
        "                        x_str = \"[\" + stone_str + \"]\"\n",
        "                    else:\n",
        "                        x_str = \" \" + stone_str + \" \"\n",
        "                line_str += x_str\n",
        "            line_str += str(y) if y >= 10 else \" \" + str(y)\n",
        "            stderr.write(line_str + \"\\n\")\n",
        "\n",
        "        pirnt_xlabel()\n",
        "        stderr.write(\"\\n\")\n",
        "\n",
        "    def showboard_file(self):\n",
        "\n",
        "        def pirnt_xlabel():\n",
        "            line_str = \"  \"\n",
        "            for x in range(BSIZE):\n",
        "                line_str += \" \" + x_labels[x] + \" \"\n",
        "            #logging.info(line_str + \"\\n\")\n",
        "            logging.info(line_str)\n",
        "\n",
        "        pirnt_xlabel()\n",
        "\n",
        "        for y in range(1, BSIZE + 1)[::-1]:  # 9, 8, ..., 1\n",
        "            line_str = str(y) if y >= 10 else \" \" + str(y)\n",
        "            for x in range(1, BSIZE + 1):\n",
        "                v = xy2ev(x, y)\n",
        "                x_str = \" . \"\n",
        "                color = self.color[v]\n",
        "                if color <= 1:\n",
        "                    stone_str = \"O\" if color == 0 else \"x\"\n",
        "                    if v == self.prev_move:\n",
        "                        x_str = \"[\" + stone_str + \"]\"\n",
        "                    else:\n",
        "                        x_str = \" \" + stone_str + \" \"\n",
        "                line_str += x_str\n",
        "            line_str += str(y) if y >= 10 else \" \" + str(y)\n",
        "            #logging.info(line_str + \"\\n\")\n",
        "            logging.info(line_str)\n",
        "\n",
        "        pirnt_xlabel()\n",
        "        #logging.info(\"\\n\")\n",
        "        logging.info(\"\\n\")\n",
        "\n",
        "    def feature(self):\n",
        "        feature_ = np.zeros((EBVCNT, FEATURE_CNT), dtype=np.float)\n",
        "        my = self.turn\n",
        "        opp = int(self.turn == 0)\n",
        "\n",
        "        feature_[:, 0] = (self.color == my)\n",
        "        feature_[:, 1] = (self.color == opp)\n",
        "        for i in range(KEEP_PREV_CNT):\n",
        "            feature_[:, (i + 1) * 2] = (self.prev_color[i] == my)\n",
        "            feature_[:, (i + 1) * 2 + 1] = (self.prev_color[i] == opp)\n",
        "        feature_[:, FEATURE_CNT - 1] = my\n",
        "\n",
        "        return feature_[rv_list, :]\n",
        "\n",
        "    def hash(self):\n",
        "        return (hash(self.color.tostring()) ^\n",
        "                hash(self.prev_color[0].tostring()) ^ self.turn)\n",
        "\n",
        "    def info(self):\n",
        "        empty_list = np.where(self.color == 2)[0]\n",
        "        cand_list = []\n",
        "        for v in empty_list:\n",
        "            if self.legal(v) and not self.eyeshape(v, self.turn):\n",
        "                cand_list.append(ev2rv(v))\n",
        "        cand_list.append(ev2rv(PASS))\n",
        "        return (self.hash(), self.move_cnt, cand_list)\n",
        "\n",
        "      # -*- coding: utf-8 -*-\n",
        "\n",
        "\n",
        "\n",
        "FILTER_CNT = 256\n",
        "\n",
        "w_wdt = 0.007\n",
        "b_wdt = 0.015\n",
        "\n",
        "class DualNetwork(object):\n",
        "\n",
        "    def get_variable(self, shape_, width_=0.007, name_=\"weight\"):\n",
        "        var = tf.get_variable(name_, shape=shape_,\n",
        "                              initializer=tf.random_normal_initializer(\n",
        "                                  mean=0, stddev=width_))\n",
        "\n",
        "        if not tf.get_variable_scope()._reuse:\n",
        "            tf.add_to_collection(\"vars_train\", var)\n",
        "\n",
        "        return var\n",
        "\n",
        "    def conv2d(self, x, w):\n",
        "        return tf.nn.conv2d(x, w, strides=[1, 1, 1, 1],\n",
        "                            padding='SAME', name=\"conv2d\")\n",
        "\n",
        "    def res_block(self, x, input_size, middle_size, output_size,\n",
        "                  dr_block=1.0, scope_name=\"res\"):\n",
        "\n",
        "        with tf.variable_scope(scope_name + \"_0\"):\n",
        "            w0 = self.get_variable([3, 3, input_size, middle_size],\n",
        "                                   w_wdt, name_=\"weight\")\n",
        "            b0 = self.get_variable([middle_size], b_wdt, name_=\"bias\")\n",
        "            conv0 = tf.nn.relu(self.conv2d(x, w0) + b0)\n",
        "        with tf.variable_scope(scope_name + \"_1\"):\n",
        "            w1 = self.get_variable([3, 3, middle_size, output_size],\n",
        "                                   w_wdt, name_=\"weight\")\n",
        "            b1 = self.get_variable([output_size], b_wdt, name_=\"bias\")\n",
        "            conv1 = tf.nn.dropout(self.conv2d(conv0, w1) + b1, dr_block)\n",
        "\n",
        "        if input_size == output_size:\n",
        "            x_add = x\n",
        "        elif input_size < output_size:\n",
        "            x_add = tf.pad(x, [[0, 0], [0, 0], [0, 0],\n",
        "                               [0, output_size - input_size]])\n",
        "        else:\n",
        "            x_add = tf.slice(x, [0, 0, 0, 0],\n",
        "                             [-1, BSIZE, BSIZE, output_size])\n",
        "\n",
        "        return tf.nn.relu(tf.add(conv1, x_add))\n",
        "\n",
        "    def model(self, x, temp=1.0, dr=1.0):\n",
        "        stdout_log(\"FILTER_CNT = {}\".format(FILTER_CNT))\n",
        "        stdout_log(\"BLOCK_CNT = {}\".format(BLOCK_CNT))\n",
        "        hi = []\n",
        "        prev_h = tf.reshape(x, [-1, BSIZE, BSIZE, FEATURE_CNT])\n",
        "\n",
        "        # residual blocks with N layers\n",
        "        for i in range(BLOCK_CNT):\n",
        "            input_size = FEATURE_CNT if i == 0 else FILTER_CNT\n",
        "            dr_block = 1 - (1 - dr) / BLOCK_CNT * i\n",
        "\n",
        "            hi.append(self.res_block(prev_h, input_size, FILTER_CNT, FILTER_CNT,\n",
        "                                     dr_block=dr_block, scope_name=\"res%d\" % i))\n",
        "            prev_h = hi[i]\n",
        "\n",
        "        # policy connection\n",
        "        with tf.variable_scope('pfc'):\n",
        "            # 1st layer\n",
        "            # [-1, BSIZE, BSIZE, FILTER_CNT] => [-1, BSIZE**2 * 2]\n",
        "            w_pfc0 = self.get_variable([1, 1, FILTER_CNT, 2],\n",
        "                                       w_wdt, name_=\"weight0\")\n",
        "            b_pfc0 = self.get_variable([BSIZE, BSIZE, 2], b_wdt, name_=\"bias0\")\n",
        "            conv_pfc0 = tf.reshape(self.conv2d(hi[BLOCK_CNT - 1], w_pfc0)\n",
        "                                   + b_pfc0, [-1, BVCNT * 2])\n",
        "\n",
        "            # 2nd layer\n",
        "            # [-1, BSIZE**2 * 2] => [-1, BSIZE**2 + 1]\n",
        "            w_pfc1 = self.get_variable([BVCNT * 2, BVCNT + 1],\n",
        "                                       w_wdt, name_=\"weight1\")\n",
        "            b_pfc1 = self.get_variable([BVCNT + 1], b_wdt, name_=\"bias1\")\n",
        "            conv_pfc1 = tf.matmul(conv_pfc0, w_pfc1) + b_pfc1\n",
        "\n",
        "            # divided by softmax temp and apply softmax\n",
        "            policy = tf.nn.softmax(tf.div(conv_pfc1, temp), name=\"policy\")\n",
        "\n",
        "        # value connection\n",
        "        with tf.variable_scope('vfc'):\n",
        "            # 1st layer\n",
        "            # [-1, BSIZE, BSIZE, FILTER_CNT] => [-1, BSIZE**2]\n",
        "            w_vfc0 = self.get_variable([1, 1, FILTER_CNT, 1],\n",
        "                                       w_wdt, name_=\"weight0\")\n",
        "            b_vfc0 = self.get_variable([BSIZE, BSIZE, 1], b_wdt, name_=\"bias0\")\n",
        "            conv_vfc0 = tf.reshape(self.conv2d(hi[BLOCK_CNT - 1], w_vfc0)\n",
        "                                   + b_vfc0, [-1, BVCNT])\n",
        "\n",
        "            # 2nd layer\n",
        "            # [-1, BSIZE**2] => [-1, 256]\n",
        "            w_vfc1 = self.get_variable([BVCNT, 256], w_wdt, name_=\"weight1\")\n",
        "            b_vfc1 = self.get_variable([256], b_wdt, name_=\"bias1\")\n",
        "            conv_vfc1 = tf.matmul(conv_vfc0, w_vfc1) + b_vfc1\n",
        "            relu_vfc1 = tf.nn.relu(conv_vfc1)\n",
        "\n",
        "            # 3rd layer\n",
        "            # [-1, 256] => [-1, 1]\n",
        "            w_vfc2 = self.get_variable([256, 1], w_wdt, name_=\"weight2\")\n",
        "            b_vfc2 = self.get_variable([1], b_wdt, name_=\"bias2\")\n",
        "            conv_vfc2 = tf.matmul(relu_vfc1, w_vfc2) + b_vfc2\n",
        "\n",
        "            # apply tanh\n",
        "            value = tf.nn.tanh(tf.reshape(conv_vfc2, [-1]), name=\"value\")\n",
        "\n",
        "        return policy, value\n",
        "\n",
        "    def create_sess(self, ckpt_path=\"\"):\n",
        "        with tf.get_default_graph().as_default():\n",
        "\n",
        "            sess_ = tf.Session(config=tf.ConfigProto(\n",
        "                allow_soft_placement=True, log_device_placement=False))\n",
        "            vars_train = tf.get_collection(\"vars_train\")\n",
        "            v_to_init = list(set(tf.global_variables()) - set(vars_train))\n",
        "\n",
        "            saver = tf.train.Saver(vars_train)\n",
        "            \n",
        "            #stdout_log('-- create_sess ckpt_path restore : {} is {}'.format(ckpt_path, os.path.isfile(ckpt_path)))\n",
        "            if ckpt_path != \"\":\n",
        "            #if os.path.exists(ckpt_path):\n",
        "                stdout_log('-- Model resotore : {}'.format(ckpt_path))\n",
        "                saver.restore(sess_, ckpt_path)\n",
        "                sess_.run(tf.variables_initializer(v_to_init))\n",
        "            else:\n",
        "                stdout_log('-- Model is Blank.\\n')\n",
        "                sess_.run(tf.global_variables_initializer())\n",
        "\n",
        "        return sess_\n",
        "\n",
        "    def save_vars(self, sess_, ckpt_path=\"\"):\n",
        "        with tf.get_default_graph().as_default():\n",
        "\n",
        "            vars_train = tf.get_collection(\"vars_train\")\n",
        "            saver = tf.train.Saver(vars_train)\n",
        "            save_path = saver.save(sess_, ckpt_path)\n",
        "\n",
        "        return save_path\n",
        "\n",
        "\n",
        "class sgf_data(object):\n",
        "\n",
        "    def __init__(self):\n",
        "        self.size = BSIZE\n",
        "        self.komi = KOMI\n",
        "        self.handicap = 0\n",
        "        self.result = 0\n",
        "        self.history = []\n",
        "        self.move_cnt = 0\n",
        "\n",
        "    def sgf2ev(self, v_sgf):\n",
        "        if len(v_sgf) != 2:\n",
        "            return (self.size + 2) ** 2\n",
        "        labels = \"abcdefghijklmnopqrs\"\n",
        "        x = labels.find(v_sgf[0]) + 1\n",
        "        y = labels.find(v_sgf[1]) + 1\n",
        "        return x + (self.size + 1 - y) * (self.size + 2)\n",
        "\n",
        "    def import_file(self, file_path):\n",
        "        f = open(file_path, encoding=\"utf-8\")\n",
        "        lines = []\n",
        "        try:\n",
        "            lines = f.readlines()\n",
        "        except:\n",
        "            print(\"reedlines exception = {}\".format(file_path))\n",
        "        for line in lines:\n",
        "            str = line.rstrip(\"\\n\")\n",
        "            while len(str) > 3:\n",
        "                open_br = str.find(\"[\")\n",
        "                close_br = str.find(\"]\")\n",
        "                if open_br < 0 or close_br < 0:\n",
        "                    break\n",
        "                elif close_br == 0:\n",
        "                    str = str[close_br + 1:]\n",
        "                    continue\n",
        "\n",
        "                key = str[0:open_br].lstrip(\";\")\n",
        "                val = str[open_br + 1:close_br]\n",
        "\n",
        "                if key == \"SZ\":\n",
        "                    self.size = int(val)\n",
        "                elif key == \"KM\":\n",
        "                    try:\n",
        "                        self.komi = float(val)\n",
        "                    except:\n",
        "                        print(\"KM error = {}\".format(file_path))\n",
        "                        self.komi = 6.5\n",
        "                        #continue\n",
        "                elif key == \"HA\":\n",
        "                    try:\n",
        "                        self.handicap = int(val)\n",
        "                    except:\n",
        "                        print(\"HA error = {}\".format(file_path))\n",
        "                        #continue\n",
        "                elif key == \"RE\":\n",
        "                    if val.find(\"B\") >= 0:\n",
        "                        self.result = 1\n",
        "                    elif val.find(\"W\") >= 0:\n",
        "                        self.result = -1\n",
        "                    else:\n",
        "                        self.result = 0\n",
        "                elif key == \"B\" or key == \"W\":\n",
        "                    self.history.append(self.sgf2ev(val))\n",
        "                    self.move_cnt += 1\n",
        "\n",
        "                str = str[close_br + 1:]\n",
        "        if self.result == 0 and len(self.history) >= 2:\n",
        "            pass_ = (self.size + 2) ** 2\n",
        "            if self.history[-1] != pass_ or self.history[-2] != pass_:\n",
        "                self.result = 1 if len(self.history) % 2 == 1 else -1\n",
        "\n",
        "\n",
        "def import_sgf(dir_path):\n",
        "    print('dir_path:', dir_path)\n",
        "    #dir_path += \"/*.sgf\"\n",
        "    file_list = glob.glob(dir_path)\n",
        "    sd_list = []\n",
        "    print(\"file_list = {}\".format(len(file_list)))\n",
        "    # b = Board()\n",
        "    fl = 0\n",
        "    for f in file_list:\n",
        "        fl += 1\n",
        "        if fl % 100 == 0:\n",
        "            print(\"import_sgf file_list count = {}\".format(fl))\n",
        "        sd_list.append(sgf_data())\n",
        "        sd_list[-1].import_file(f)\n",
        "\n",
        "#         b.clear()\n",
        "#         for v in sd_list[-1].history:\n",
        "#             err = b.play(v, not_fill_eye=False)\n",
        "#             if err:\n",
        "#                 stderr.write(\"file %d\\n\" % len(sd_list))\n",
        "#                 b.showboard()\n",
        "#                 stderr.write(\"move=(%d,%d)\\n\" % ev2xy(v))\n",
        "#                 raw_input()\n",
        "\n",
        "#         if len(sd_list) % 5000 == 0:\n",
        "#             stderr.write(\".\")\n",
        "\n",
        "    return sd_list\n",
        "\n",
        "\n",
        "def sgf2feed(sgf_list):\n",
        "    total_cnt = 0\n",
        "    for s in sgf_list:\n",
        "        if s.size != BSIZE or s.handicap != 0 or s.result == 0:\n",
        "            continue\n",
        "        total_cnt += s.move_cnt\n",
        "\n",
        "    feature = np.zeros((total_cnt, BVCNT, FEATURE_CNT), dtype=np.uint8)\n",
        "    move = np.zeros((total_cnt, BVCNT + 1), dtype=np.uint8)\n",
        "    result = np.zeros((total_cnt), dtype=np.int8)\n",
        "\n",
        "    \n",
        "    train_idx = 0\n",
        "    b = Board()\n",
        "    sf_count = 0\n",
        "    stdout_log(\"sgf2feed sgf_list = {}\".format(len(sgf_list)))\n",
        "    for s in sgf_list:\n",
        "        sf_count += 1\n",
        "        if sf_count % 100 == 0:\n",
        "            print(\"sgf2feed sgf_list count = {}\".format(sf_count))\n",
        "        if s.size != BSIZE or s.handicap != 0 or s.result == 0:\n",
        "            continue\n",
        "        b.clear()\n",
        "        for v in s.history:\n",
        "            feature[train_idx] = b.feature()\n",
        "            #logging.info(\"feature = {}\".format(feature[train_idx]))\n",
        "            move[train_idx, ev2rv(v)] = 1\n",
        "            #logging.info(\"move = {}\".format(move[train_idx]))\n",
        "            result[train_idx] = s.result * (2 * b.turn - 1)\n",
        "            #logging.info(\"result = {}\".format(result[train_idx]))\n",
        "\n",
        "            b.play(v, False)\n",
        "            train_idx += 1\n",
        "\n",
        "    return feature, move, result\n",
        "\n",
        "\n",
        "rnd_array = [np.arange(BVCNT + 1)]\n",
        "for i in range(1, 8):\n",
        "    rnd_array.append(rnd_array[i - 1])\n",
        "    rot_array = rnd_array[i][:BVCNT].reshape(BSIZE, BSIZE)\n",
        "    if i % 2 == 0:\n",
        "        rot_array = rot_array.transpose(1, 0)\n",
        "    else:\n",
        "        rot_array = rot_array[::-1, :]\n",
        "    rnd_array[i][:BVCNT] = rot_array.reshape(BVCNT)\n",
        "\n",
        "\n",
        "class Feed(object):\n",
        "\n",
        "    def __init__(self, f_, m_, r_):\n",
        "        self._feature = f_\n",
        "        self._move = m_\n",
        "        self._result = r_\n",
        "        self.size = self._feature.shape[0]\n",
        "        self._idx = 0\n",
        "        self._perm = np.arange(self.size)\n",
        "        np.random.shuffle(self._perm)\n",
        "\n",
        "        #logging.info(\"self._feature = {}\".format(f_))\n",
        "        logging.info(\"self._move shape = {}\".format(m_.shape))\n",
        "        #logging.info(\"self._move = {}\".format(m_))\n",
        "        logging.info(\"self._result shape = {}\".format(r_.shape))\n",
        "        #logging.info(\"self._result = {}\".format(r_))\n",
        "        logging.info(\"self.feature shape = {}\".format(self._feature.shape))\n",
        "        logging.info(\"self.size = {}\".format(self._feature.shape[0]))\n",
        "        logging.info(\"self._idx = {}\".format(self._idx))\n",
        "        logging.info(\"self._perm shape = {}\".format(self._perm.shape))\n",
        "        #logging.info(\"self._perm = {}\".format(self._perm))\n",
        "\n",
        "    def next_batch(self, batch_size=128):\n",
        "        if self._idx > self.size:\n",
        "            np.random.shuffle(self._perm)\n",
        "            self._idx = 0\n",
        "        start = self._idx\n",
        "        self._idx += batch_size\n",
        "        end = self._idx\n",
        "\n",
        "        #logging.info(\"start = {} end = {}\".format(start, end))\n",
        "        #logging.info(\"self._feature = {}\".format(self._feature))\n",
        "\n",
        "        rnd_cnt = np.random.choice(np.arange(8))\n",
        "\n",
        "        f_batch = self._feature[self._perm[start:end]]  # slice for mini-batch\n",
        "        #logging.info('f_batch_1 = {}'.format(f_batch))\n",
        "        \n",
        "        f_batch = f_batch[:, rnd_array[rnd_cnt][:BVCNT]].astype(np.float32)\n",
        "        #logging.info('f_batch_2 = {}'.format(f_batch))\n",
        "        \n",
        "        m_batch = self._move[self._perm[start:end]]  # slice for mini-batch\n",
        "        #logging.info('m_batch_1 = {}'.format(m_batch))\n",
        "        \n",
        "        m_batch = m_batch[:, rnd_array[rnd_cnt]].astype(np.float32)\n",
        "        #logging.info('f_batch_2 = {}'.format(m_batch))\n",
        "        \n",
        "        r_batch = self._result[self._perm[start:end]].astype(np.float32)\n",
        "        #logging.info('r_batch = {}'.format(r_batch))\n",
        "\n",
        "        return f_batch, m_batch, r_batch\n",
        "\n",
        "\n",
        "def average_gradients(tower_grads):\n",
        "    average_grads = []\n",
        "    for grad_and_vars in zip(*tower_grads):\n",
        "\n",
        "        grads = []\n",
        "        for g, _ in grad_and_vars:\n",
        "            grads.append(tf.expand_dims(g, 0))\n",
        "\n",
        "        grad = tf.reduce_mean(tf.concat(grads, 0), 0)\n",
        "        v = grad_and_vars[0][1]\n",
        "        average_grads.append((grad, v))\n",
        "\n",
        "    return average_grads\n",
        "\n",
        "\n",
        "def learn(lr_=1e-4, dr_=0.7, sgf_dir=\"sgf/\", use_gpu=True, gpu_cnt=1, model_name=\"\"):\n",
        "    \n",
        "    device_name = \"gpu\" if use_gpu else \"cpu\"\n",
        "    with tf.get_default_graph().as_default(), tf.device(\"/cpu:0\"):\n",
        "\n",
        "        # placeholders\n",
        "        f_list = []\n",
        "        r_list = []\n",
        "        m_list = []\n",
        "        for gpu_idx in range(gpu_cnt):\n",
        "            f_list.append(tf.placeholder(\n",
        "                \"float\", shape=[None, BVCNT, FEATURE_CNT],\n",
        "                name=\"feature_%d\" % gpu_idx))\n",
        "            r_list.append(tf.placeholder(\n",
        "                \"float\", shape=[None], name=\"result_%d\" % gpu_idx))\n",
        "            m_list.append(tf.placeholder(\n",
        "                \"float\", shape=[None, BVCNT + 1], name=\"move_%d\" % gpu_idx))\n",
        "\n",
        "        lr = tf.placeholder(tf.float32, shape=[], name=\"learning_rate\")\n",
        "\n",
        "        opt = tf.train.AdamOptimizer(lr)\n",
        "        dn = DualNetwork()\n",
        "\n",
        "        # compute and apply gradients\n",
        "        tower_grads = []\n",
        "        with tf.variable_scope(tf.get_variable_scope()):\n",
        "            for gpu_idx in range(gpu_cnt):\n",
        "                with tf.device(\"/%s:%d\" % (device_name, gpu_idx)):\n",
        "\n",
        "                    policy_, value_ = dn.model(\n",
        "                        f_list[gpu_idx], temp=1.0, dr=dr_)\n",
        "                    policy_ = tf.clip_by_value(policy_, 1e-6, 1)\n",
        "\n",
        "                    loss_p = -tf.reduce_mean(tf.log(\n",
        "                        tf.reduce_sum(tf.multiply(m_list[gpu_idx], policy_), 1)))\n",
        "                    loss_v = tf.reduce_mean(\n",
        "                        tf.square(tf.subtract(value_, r_list[gpu_idx])))\n",
        "                    if gpu_idx == 0:\n",
        "                        vars_train = tf.get_collection(\"vars_train\")\n",
        "                    loss_l2 = tf.add_n([tf.nn.l2_loss(v) for v in vars_train])\n",
        "                    loss = loss_p + 0.05 * loss_v + 1e-4 * loss_l2\n",
        "\n",
        "                    tower_grads.append(opt.compute_gradients(loss))\n",
        "                    tf.get_variable_scope().reuse_variables()\n",
        "\n",
        "        train_op = opt.apply_gradients(average_gradients(tower_grads))\n",
        "\n",
        "        # calculate accuracy\n",
        "        with tf.variable_scope(tf.get_variable_scope(), reuse=True):\n",
        "            with tf.device(\"/%s:0\" % device_name):\n",
        "                f_acc = tf.placeholder(\n",
        "                    \"float\", shape=[None, BVCNT, FEATURE_CNT], name=\"feature_acc\")\n",
        "                m_acc = tf.placeholder(\n",
        "                    \"float\", shape=[None, BVCNT + 1], name=\"move_acc\")\n",
        "                r_acc = tf.placeholder(\n",
        "                    \"float\", shape=[None], name=\"result_acc\")\n",
        "\n",
        "                p_, v_ = dn.model(f_acc, temp=1.0, dr=1.0)\n",
        "                prediction = tf.equal(tf.reduce_max(p_, 1),\n",
        "                                      tf.reduce_max(tf.multiply(p_, m_acc), 1))\n",
        "                accuracy_p = tf.reduce_mean(tf.cast(prediction, \"float\"))\n",
        "                accuracy_v = tf.reduce_mean(tf.square(tf.subtract(v_, r_acc)))\n",
        "                accuracy = (accuracy_p, accuracy_v)\n",
        "\n",
        "        ckpt = tf.train.get_checkpoint_state(sgf_dir)\n",
        "        \n",
        "        #last_model = ckpt.model_checkpoint_path # 最後に保存したmodelへのパス\n",
        "        \n",
        "        last_model = sgf_dir + model_name\n",
        "        stdout_log(\"last_model = {}\".format(last_model))\n",
        "        \n",
        "        if os.path.isfile(last_model + \".data-00000-of-00001\"):    \n",
        "            sess = dn.create_sess(last_model)\n",
        "        else:\n",
        "            sess = dn.create_sess()\n",
        "\n",
        "    # load sgf and convert to feed\n",
        "    stdout_log(\"----------- start training-------------\")\n",
        "\n",
        "    limit_time = time.time() + STEP_TIME - SAVE_TIME\n",
        "    endt = time.ctime(limit_time + JST)\n",
        "    cnvt = time.strptime(endt)\n",
        "    \n",
        "    stdout_log(\"STEP_TIME = {}\".format(STEP_TIME))\n",
        "    stdout_log(\"SAVE_TIME = {}\".format(SAVE_TIME))\n",
        "    stdout_log(\"END_TIME = {}\".format(time.strftime(\"%Y/%m/%d %H:%M:%S\", cnvt)))\n",
        "    \n",
        "    #with open(sgf_dir + 'trained.txt', mode='w') as f:\n",
        "    #    trained = [str(0), str(0), str(0), str(0)]\n",
        "    #    f.write('\\n'.join(trained)) \n",
        "    #with open(sgf_dir + 'epoch_count.txt', mode='w') as f:\n",
        "    #    t = str(0)\n",
        "    #    f.write(t)    \n",
        "    \n",
        "    #前回実行時の状態を復元\n",
        "    with open(sgf_dir + model_name + '_' + 'trained.txt') as f:\n",
        "        l_strip = [s.strip() for s in f.readlines()]\n",
        "        start_epoch_idx = int(l_strip[0])\n",
        "        start_step_idx = int(l_strip[1])\n",
        "        global_step_idx = int(l_strip[2])\n",
        "        no = int(l_strip[3])\n",
        "\n",
        "        stdout_log(\"start_epoch_idx = {}\".format(start_epoch_idx))\n",
        "        stdout_log(\"start_step_idx = {}\".format(start_step_idx))\n",
        "        stdout_log(\"global_step_idx = {}\".format(global_step_idx))\n",
        "        stdout_log(\"feed_file_no = {}\".format(no))\n",
        "        \n",
        "    with open(sgf_dir + model_name + '_' + 'epoch_count.txt') as f:\n",
        "        epoch_count = int(f.read())        \n",
        "        stdout_log(\"epoch_count = {}\".format(epoch_count))\n",
        "\n",
        "    stdout_log(\"Converting ...\")\n",
        "    start_time = time.time()\n",
        "    \n",
        "    with open(sgf_dir + 'feed_n_17979696.pickle', mode='rb') as f:\n",
        "        feed_n = pickle.load(f)\n",
        "        stdout_log(\"loaded feed_n count = {}\".format(len(feed_n)))\n",
        "        stdout_log(\"loaded feed_n = {}\".format(feed_n))\n",
        "\n",
        "    # 全feed数\n",
        "    feed_cnt = 0\n",
        "\n",
        "    # 読み込みファイルNoの設定)(0～283)\n",
        "    n = 283\n",
        "    stdout_log(\"feed_files No. = {}\".format(n))\n",
        "    \n",
        "    feed_cnt = feed_n[n]\n",
        "    \n",
        "    #読み込むファイル番号を保持するリスト\n",
        "    feed_files = list(range(n + 1))\n",
        "\n",
        "    #pickleファイルのfeed数\n",
        "    feed_n_1 = []\n",
        "    for i in range(len(feed_n)):\n",
        "        if i == 0:\n",
        "            feed_n_1.append(feed_n[0])\n",
        "            continue\n",
        "        else:\n",
        "            feed_n_1.append(feed_n[i] - feed_n[i - 1])\n",
        "\n",
        "    stdout_log(\"loaded feed_n_1 count = {}\".format(len(feed_n_1)))\n",
        "    stdout_log(\"loaded feed_n_1 = {}\".format(feed_n_1))\n",
        "\n",
        "    # learning settings\n",
        "    batch_cnt = 512\n",
        "    #batch_cnt = 2000\n",
        "    #total_epochs = 8 * 5\n",
        "    total_epochs = 10\n",
        "    epoch_steps = feed_cnt // (batch_cnt * gpu_cnt)\n",
        "    total_steps = total_epochs * epoch_steps\n",
        "    #global_step_idx = 0\n",
        "    learning_rate = lr_\n",
        "\n",
        "    stdout_log(\"feed_cnt = %d\" % (feed_cnt))\n",
        "    stdout_log(\"batch_cnt = %d\" % (batch_cnt))\n",
        "    stdout_log(\"total_epochs = %d\" % (total_epochs))\n",
        "    stdout_log(\"epoch_steps = %d\" % (epoch_steps))\n",
        "    stdout_log(\"total_steps = %d\" % (total_steps))\n",
        "    stdout_log(\"learning rate = %.1g\" % (learning_rate))\n",
        "    stdout_log(\"model_name = %s\" % (model_name))\n",
        "    \n",
        "    # training\n",
        "    for epoch_idx in range(start_epoch_idx, total_epochs):\n",
        "        if epoch_idx > 0 and (epoch_idx - 8) % 8 == 0:\n",
        "            learning_rate *= 0.5\n",
        "            stdout_log(\"learning rate=%.1g\" % (learning_rate))\n",
        "\n",
        "        \n",
        "        #シャッフルした後のfeedの読み込み順の累積\n",
        "        feed_n_shuffled = []\n",
        "        \n",
        "        #新しいepochに入った場合はシャッフルする\n",
        "        if epoch_idx > epoch_count - 1:\n",
        "            stdout_log(\"shuffling...\")\n",
        "            epoch_count = epoch_idx + 1\n",
        "\n",
        "            #読み込みファイルNo.の順番(ランダム)\n",
        "            random.shuffle(feed_files)\n",
        "            stdout_log(\"feed_files_shuffled = {}\".format(feed_files))\n",
        "\n",
        "            for i in range(len(feed_files)):\n",
        "                if i == 0:\n",
        "                    feed_n_shuffled.append(feed_n_1[feed_files[i]])\n",
        "                    continue\n",
        "                else:\n",
        "                    feed_n_shuffled.append(feed_n_1[feed_files[i]] + feed_n_shuffled[i - 1])\n",
        "\n",
        "            a = []\n",
        "            for i in range(len(feed_files)):\n",
        "                a.append(feed_n_1[feed_files[i]])\n",
        "\n",
        "            stdout_log(\"feed_n_1 = {}\".format(a))    \n",
        "            stdout_log(\"feed_n_shuffled = {}\".format(feed_n_shuffled))\n",
        "\n",
        "\n",
        "            with open(sgf_dir + model_name + '_' + 'epoch_count.txt', mode='w') as f:\n",
        "                f.write(str(epoch_count)) \n",
        "            with open(sgf_dir + model_name + '_' + 'feed_files.txt', mode='w') as f:\n",
        "                a_str = [str(num) for num in feed_files]\n",
        "                f.write('\\n'.join(a_str)) \n",
        "            with open(sgf_dir + model_name + '_' + 'feed_n_shuffled.txt', mode='w') as f:\n",
        "                a_str = [str(num) for num in feed_n_shuffled]\n",
        "                f.write('\\n'.join(a_str)) \n",
        "        else:\n",
        "            with open(sgf_dir + model_name + '_' + 'feed_files.txt', mode='r') as f:\n",
        "                feed_files = [int(s.strip()) for s in f.readlines()]\n",
        "                stdout_log(\"feed_files reload = {}\".format(feed_files))\n",
        "            with open(sgf_dir + model_name + '_' + 'feed_n_shuffled.txt', mode='r') as f:\n",
        "                feed_n_shuffled = [int(s.strip()) for s in f.readlines()]\n",
        "                stdout_log(\"feed_n_shuffled reload = {}\".format(feed_n_shuffled))\n",
        "\n",
        "        \n",
        "        file_number = feed_files[no]\n",
        "        \n",
        "        f = open(sgf_dir + 'feed_train_' + str(file_number) + '.pickle','rb')\n",
        "        feed_train = pickle.load(f) \n",
        "        \n",
        "        stdout_log('training : feed_train_' + str(file_number) + '.pickle')\n",
        "        stdout_log(\"training : steps = {}\".format(feed_train[0].size // batch_cnt + 1))\n",
        "        \n",
        "        for step_idx in range(start_step_idx, epoch_steps):\n",
        "            feed_dict_ = {}\n",
        "            feed_dict_[lr] = learning_rate\n",
        "            for gpu_idx in range(gpu_cnt):\n",
        "                \n",
        "                if feed_n_shuffled[no]  <= step_idx * batch_cnt:\n",
        "                    stdout_log('training : step_idx = {}'.format(step_idx))\n",
        "                    no += 1\n",
        "                    if no > n:\n",
        "                        no = 0\n",
        "                    file_number = feed_files[no]\n",
        "                    with open(sgf_dir + 'feed_train_' + str(file_number) + '.pickle','rb') as f:\n",
        "                        feed_train = pickle.load(f) \n",
        "                        stdout_log('training : feed_train_' + str(file_number) + '.pickle')\n",
        "                        stdout_log(\"training : steps = {}\".format(feed_train[0].size // batch_cnt + 1))\n",
        "                \n",
        "                batch = feed_train[0].next_batch(batch_cnt)\n",
        "                feed_dict_[f_list[gpu_idx]] = np.array(batch[0])\n",
        "                feed_dict_[m_list[gpu_idx]] = np.array(batch[1])\n",
        "                feed_dict_[r_list[gpu_idx]] = np.array(batch[2])\n",
        "\n",
        "            sess.run(train_op, feed_dict = feed_dict_)\n",
        "            \n",
        "            global_step_idx += 1\n",
        "\n",
        "            if global_step_idx % 100 == 0:\n",
        "                stdout_log(\"global_steps... = {}\".format(global_step_idx))\n",
        "\n",
        "            if global_step_idx % (total_steps // 1000) == 0:\n",
        "                progress_now = float(global_step_idx) / total_steps * 100\n",
        "                str_log = \"progress: %03.2f[%%] \" % (progress_now)\n",
        "\n",
        "                elapsed_time = time.time() - start_time\n",
        "                str_log += \"%03.1f\" % (elapsed_time) + \"[sec]\"\n",
        "                stdout_log(\"%s\" % (str_log))\n",
        "                start_time = time.time()\n",
        "                \n",
        "                dn.save_vars(sess, sgf_dir + model_name)\n",
        "                stdout_log(\"save model.ckpt\")\n",
        "\n",
        "                with open(sgf_dir + model_name + '_' + 'trained.txt', mode='w') as f:\n",
        "                    trained = [str(epoch_idx), str(step_idx + 1), str(global_step_idx), str(no)]\n",
        "                    f.write('\\n'.join(trained))    \n",
        "                    stdout_log(\"epoch_idx = {}, step_idx = {}, gloal_step_idx = {}, feed_file_no = {}\".format(epoch_idx, step_idx + 1, global_step_idx, no))\n",
        "            \n",
        "            if time.time() > limit_time or (global_step_idx % (total_steps // 200) == 0):\n",
        "            #if global_step_idx % 1 == 0:\n",
        "\n",
        "                with open(sgf_dir + 'feed_test.pickle', mode='rb') as f:\n",
        "                    feed_test = pickle.load(f)\n",
        "                    feed = [feed_train, feed_test]\n",
        "                    stdout_log(\"loaded feed_test.\")\n",
        "\n",
        "                str_log = \"\"\n",
        "                # str_summary = \"%3.3f\" % (float(global_step_idx) / total_steps * 100)\n",
        "                acc_steps = feed[1][0].size // batch_cnt\n",
        "                np.random.shuffle(feed[0][0]._perm)\n",
        "                for i in range(2):\n",
        "                    acc_str = \"train\" if i == 0 else \"test \"\n",
        "                    acc_sum = [0.0, 0.0]\n",
        "\n",
        "                    for _ in range(acc_steps):\n",
        "                        acc_batch = feed[i][0].next_batch(batch_cnt)\n",
        "                        accur = sess.run(accuracy, feed_dict={f_acc: acc_batch[0], m_acc: acc_batch[1], r_acc: acc_batch[2]})\n",
        "                        acc_sum[0] += accur[0]\n",
        "                        acc_sum[1] += accur[1]\n",
        "\n",
        "                    str_log += \"%s: policy=%3.5f[%%]  value=%.5f \" % (acc_str, acc_sum[0] / acc_steps * 100, acc_sum[1] / acc_steps / 2)\n",
        "                    # str_summary += \"\\t%3.3f\\t%3.3f\" \\\n",
        "                    #     % (acc_sum[0] / acc_steps * 100,\n",
        "                    #        acc_sum[1] / acc_steps / 2)\n",
        "\n",
        "                stdout_log(\"%s\" % (str_log))\n",
        "                stdout_log(\"save model.ckpt\")\n",
        "                dn.save_vars(sess, sgf_dir + model_name)\n",
        "                \n",
        "                with open(sgf_dir + model_name + '.txt', mode='w') as f:\n",
        "                    now_dt = datetime.now()\n",
        "                    dt_jst = now_dt + timedelta(minutes=540)\n",
        "                    str_datetime = datetime.strftime(dt_jst, \"%Y/%m/%d-%H:%M:%S\")\n",
        "                    f.write(str_datetime + ' ' +  model_name + ' ' + str_log)\n",
        "                \n",
        "                #with open(sgf_dir + 'trained.txt', mode='w') as f:\n",
        "                #    trained = [str(epoch_idx), str(step_idx + 1), str(global_step_idx), str(no)]\n",
        "                #    f.write('\\n'.join(trained))    \n",
        "                #    stdout_log(\"epoch_idx = {}, step_idx = {}, gloal_step_idx = {}, feed_file_no = {}\".format(epoch_idx, step_idx + 1, global_step_idx, no))                         \n",
        "                \n",
        "                if time.time() > limit_time:\n",
        "                    stdout_log(\"★★★ Time Limit....\")\n",
        "                    return\n",
        "            \n",
        "        #１epoch終了したので、ファイルNoを0にする。\n",
        "        no = 0\n",
        "        start_step_idx = 0\n",
        "        \n",
        "    dn.save_vars(sess, sgf_dir + model_name)\n",
        "\n",
        "        \n",
        "import sys\n",
        "sys.path.append('/content/drive/My Drive/')\n",
        "\n",
        "from learn import Feed\n",
        "\n",
        "STEP_TIME = 3600 * 12\n",
        "SAVE_TIME = 60 * 26\n",
        "JST = 3600 * 9\n",
        "\n",
        "BLOCK_CNT = 8\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    model = \"model_256_\" + str(BLOCK_CNT) + \".ckpt\"\n",
        "    learn(3e-4, 0.5, sgf_dir=r\"/content/drive/My Drive/\", use_gpu=True, gpu_cnt=1, model_name=model)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "2019/04/22-07:36:55 FILTER_CNT = 256\n",
            "2019/04/22-07:36:55 BLOCK_CNT = 8\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Colocations handled automatically by placer.\n",
            "WARNING:tensorflow:From <ipython-input-1-72dbc64fea1e>:477: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
            "WARNING:tensorflow:From <ipython-input-1-72dbc64fea1e>:523: div (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Deprecated in favor of operator or tf.math.divide.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use tf.cast instead.\n",
            "2019/04/22-07:36:56 FILTER_CNT = 256\n",
            "2019/04/22-07:36:56 BLOCK_CNT = 8\n",
            "2019/04/22-07:36:57 last_model = /content/drive/My Drive/model_256_8.ckpt\n",
            "2019/04/22-07:36:58 -- Model resotore : /content/drive/My Drive/model_256_8.ckpt\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.6/dist-packages/tensorflow/python/training/saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use standard file APIs to check for files with this prefix.\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/model_256_8.ckpt\n",
            "2019/04/22-07:37:01 ----------- start training-------------\n",
            "2019/04/22-07:37:01 STEP_TIME = 43200\n",
            "2019/04/22-07:37:01 SAVE_TIME = 1560\n",
            "2019/04/22-07:37:01 END_TIME = 2019/04/22 19:11:01\n",
            "2019/04/22-07:37:01 start_epoch_idx = 0\n",
            "2019/04/22-07:37:01 start_step_idx = 31590\n",
            "2019/04/22-07:37:01 global_step_idx = 31590\n",
            "2019/04/22-07:37:01 feed_file_no = 255\n",
            "2019/04/22-07:37:02 epoch_count = 1\n",
            "2019/04/22-07:37:02 Converting ...\n",
            "2019/04/22-07:37:02 loaded feed_n count = 284\n",
            "2019/04/22-07:37:02 loaded feed_n = [68940, 140162, 208293, 279013, 348248, 417764, 486114, 555979, 626402, 696984, 765989, 835016, 904985, 973293, 1043663, 1112155, 1181182, 1250203, 1321028, 1391217, 1461878, 1529255, 1598316, 1669832, 1738130, 1806060, 1875742, 1949450, 2019880, 2094046, 2161714, 2234320, 2302378, 2370111, 2439838, 2508882, 2576728, 2646245, 2717012, 2784860, 2854565, 2923449, 2992397, 3054822, 3118521, 3180247, 3240828, 3301534, 3365778, 3425928, 3489024, 3552528, 3616995, 3682815, 3747375, 3812032, 3870186, 3931554, 3989996, 4049917, 4113750, 4176684, 4241875, 4306412, 4370725, 4433711, 4496363, 4562894, 4626253, 4687553, 4743856, 4805300, 4865427, 4928906, 4990493, 5051893, 5112441, 5174701, 5239972, 5302861, 5366799, 5429273, 5491025, 5554221, 5619468, 5685620, 5746792, 5804722, 5863622, 5925998, 5984562, 6045089, 6108353, 6163608, 6211741, 6274803, 6336993, 6402328, 6464976, 6525395, 6585135, 6642591, 6705951, 6769166, 6826975, 6881821, 6940943, 7003517, 7063543, 7124745, 7187455, 7247333, 7304810, 7364977, 7426430, 7479865, 7544769, 7606889, 7670711, 7734905, 7793230, 7855137, 7920603, 7983162, 8046217, 8111027, 8174515, 8229454, 8289027, 8352808, 8415891, 8480601, 8545037, 8609470, 8666580, 8730017, 8791869, 8853385, 8912433, 8971722, 9034044, 9097492, 9156030, 9219175, 9282010, 9345611, 9408067, 9473131, 9536687, 9598994, 9660904, 9723486, 9789299, 9853632, 9917713, 9980600, 10044448, 10109103, 10172398, 10235217, 10297193, 10360282, 10421769, 10483809, 10546997, 10607611, 10671740, 10733823, 10795062, 10847450, 10909757, 10970350, 11033328, 11092625, 11162806, 11226163, 11286279, 11349702, 11411660, 11474829, 11539862, 11603156, 11669510, 11737754, 11796678, 11857717, 11920624, 11980214, 12044321, 12108504, 12173906, 12237703, 12301216, 12365099, 12430495, 12497445, 12562728, 12627858, 12691939, 12754734, 12816629, 12875683, 12939298, 13004301, 13059365, 13116762, 13183861, 13251935, 13316941, 13378531, 13441914, 13504593, 13561988, 13625216, 13688548, 13750566, 13810883, 13872928, 13931358, 13991278, 14050055, 14109736, 14172109, 14231967, 14293045, 14351391, 14405464, 14468208, 14533435, 14596173, 14653718, 14716675, 14775722, 14841165, 14906313, 14970889, 15032569, 15095854, 15160261, 15225984, 15290320, 15351996, 15409646, 15465410, 15532156, 15595794, 15655088, 15717953, 15780612, 15837124, 15893160, 15955196, 16021798, 16088672, 16151907, 16218425, 16280340, 16345828, 16409330, 16465152, 16529299, 16594297, 16658012, 16722948, 16786718, 16849903, 16912091, 16978550, 17042728, 17107075, 17168574, 17229457, 17292616, 17357024, 17418076, 17478971, 17538659, 17599715, 17656198, 17720275, 17783689, 17846911, 17905734, 17979696]\n",
            "2019/04/22-07:37:02 feed_files No. = 283\n",
            "2019/04/22-07:37:02 loaded feed_n_1 count = 284\n",
            "2019/04/22-07:37:02 loaded feed_n_1 = [68940, 71222, 68131, 70720, 69235, 69516, 68350, 69865, 70423, 70582, 69005, 69027, 69969, 68308, 70370, 68492, 69027, 69021, 70825, 70189, 70661, 67377, 69061, 71516, 68298, 67930, 69682, 73708, 70430, 74166, 67668, 72606, 68058, 67733, 69727, 69044, 67846, 69517, 70767, 67848, 69705, 68884, 68948, 62425, 63699, 61726, 60581, 60706, 64244, 60150, 63096, 63504, 64467, 65820, 64560, 64657, 58154, 61368, 58442, 59921, 63833, 62934, 65191, 64537, 64313, 62986, 62652, 66531, 63359, 61300, 56303, 61444, 60127, 63479, 61587, 61400, 60548, 62260, 65271, 62889, 63938, 62474, 61752, 63196, 65247, 66152, 61172, 57930, 58900, 62376, 58564, 60527, 63264, 55255, 48133, 63062, 62190, 65335, 62648, 60419, 59740, 57456, 63360, 63215, 57809, 54846, 59122, 62574, 60026, 61202, 62710, 59878, 57477, 60167, 61453, 53435, 64904, 62120, 63822, 64194, 58325, 61907, 65466, 62559, 63055, 64810, 63488, 54939, 59573, 63781, 63083, 64710, 64436, 64433, 57110, 63437, 61852, 61516, 59048, 59289, 62322, 63448, 58538, 63145, 62835, 63601, 62456, 65064, 63556, 62307, 61910, 62582, 65813, 64333, 64081, 62887, 63848, 64655, 63295, 62819, 61976, 63089, 61487, 62040, 63188, 60614, 64129, 62083, 61239, 52388, 62307, 60593, 62978, 59297, 70181, 63357, 60116, 63423, 61958, 63169, 65033, 63294, 66354, 68244, 58924, 61039, 62907, 59590, 64107, 64183, 65402, 63797, 63513, 63883, 65396, 66950, 65283, 65130, 64081, 62795, 61895, 59054, 63615, 65003, 55064, 57397, 67099, 68074, 65006, 61590, 63383, 62679, 57395, 63228, 63332, 62018, 60317, 62045, 58430, 59920, 58777, 59681, 62373, 59858, 61078, 58346, 54073, 62744, 65227, 62738, 57545, 62957, 59047, 65443, 65148, 64576, 61680, 63285, 64407, 65723, 64336, 61676, 57650, 55764, 66746, 63638, 59294, 62865, 62659, 56512, 56036, 62036, 66602, 66874, 63235, 66518, 61915, 65488, 63502, 55822, 64147, 64998, 63715, 64936, 63770, 63185, 62188, 66459, 64178, 64347, 61499, 60883, 63159, 64408, 61052, 60895, 59688, 61056, 56483, 64077, 63414, 63222, 58823, 73962]\n",
            "2019/04/22-07:37:02 feed_cnt = 17979696\n",
            "2019/04/22-07:37:02 batch_cnt = 512\n",
            "2019/04/22-07:37:02 total_epochs = 10\n",
            "2019/04/22-07:37:02 epoch_steps = 35116\n",
            "2019/04/22-07:37:02 total_steps = 351160\n",
            "2019/04/22-07:37:02 learning rate = 0.0003\n",
            "2019/04/22-07:37:02 model_name = model_256_8.ckpt\n",
            "2019/04/22-07:37:03 feed_files reload = [12, 82, 231, 229, 217, 260, 8, 188, 156, 180, 274, 216, 6, 253, 144, 107, 183, 261, 254, 100, 155, 56, 236, 118, 2, 18, 205, 37, 19, 239, 40, 79, 258, 22, 267, 181, 80, 160, 1, 212, 226, 88, 102, 159, 190, 165, 272, 187, 81, 78, 270, 50, 264, 275, 28, 250, 45, 55, 54, 110, 276, 265, 38, 146, 204, 277, 268, 83, 70, 41, 132, 36, 112, 58, 203, 199, 57, 134, 60, 163, 48, 116, 111, 114, 77, 220, 125, 67, 202, 157, 69, 147, 120, 105, 140, 26, 11, 31, 32, 123, 98, 237, 208, 223, 192, 167, 283, 30, 13, 49, 34, 174, 119, 10, 33, 131, 95, 240, 177, 137, 150, 61, 257, 166, 263, 281, 200, 0, 93, 164, 243, 218, 65, 84, 145, 171, 59, 172, 154, 15, 94, 141, 176, 86, 278, 232, 209, 53, 249, 245, 182, 195, 198, 42, 255, 85, 16, 97, 129, 185, 266, 169, 76, 224, 29, 124, 25, 282, 51, 133, 104, 7, 46, 71, 43, 128, 193, 63, 259, 66, 256, 151, 189, 241, 91, 39, 222, 87, 175, 201, 135, 219, 238, 215, 197, 252, 101, 138, 153, 206, 143, 92, 186, 191, 225, 3, 152, 62, 68, 158, 244, 109, 271, 126, 178, 247, 23, 280, 9, 228, 227, 170, 113, 75, 213, 149, 74, 21, 47, 262, 279, 142, 4, 139, 161, 246, 27, 44, 90, 103, 115, 35, 108, 162, 179, 173, 234, 136, 106, 221, 251, 5, 127, 248, 168, 211, 72, 122, 89, 207, 20, 273, 117, 210, 64, 96, 242, 99, 130, 24, 235, 121, 269, 214, 73, 17, 184, 196, 148, 233, 14, 52, 230, 194]\n",
            "2019/04/22-07:37:03 feed_n_shuffled reload = [69969, 131721, 194678, 257416, 319461, 383608, 454031, 518138, 581986, 647019, 708071, 768388, 836738, 903612, 966447, 1029021, 1097265, 1162263, 1225498, 1285238, 1348125, 1406279, 1467959, 1531781, 1599912, 1670737, 1728134, 1797651, 1867840, 1933563, 2003268, 2066157, 2129659, 2198720, 2265179, 2328473, 2392411, 2454387, 2525609, 2583004, 2637077, 2695977, 2759337, 2822156, 2887558, 2948172, 3011331, 3070921, 3133395, 3198666, 3260165, 3323261, 3387031, 3447926, 3518356, 3574392, 3636118, 3700775, 3765335, 3828045, 3887733, 3950918, 4021685, 4084141, 4139205, 4200261, 4264439, 4327635, 4383938, 4452822, 4517258, 4585104, 4642581, 4701023, 4766026, 4828821, 4890189, 4947299, 5011132, 5073172, 5137416, 5202320, 5262198, 5323651, 5385911, 5444688, 5509498, 5576029, 5639644, 5704299, 5765599, 5830663, 5888988, 5943834, 6006156, 6075838, 6144865, 6217471, 6285529, 6348088, 6410736, 6474021, 6539027, 6598885, 6662398, 6724481, 6798443, 6866111, 6934419, 6994569, 7064296, 7134477, 7198671, 7267676, 7335409, 7400119, 7463181, 7527517, 7590940, 7652456, 7714366, 7777300, 7842788, 7906917, 7971853, 8035075, 8096970, 8165910, 8221165, 8284353, 8340117, 8398547, 8461533, 8526780, 8590381, 8650974, 8710895, 8773873, 8837954, 8906446, 8954579, 9018027, 9078143, 9139315, 9195798, 9254845, 9316435, 9382255, 9438767, 9502405, 9568759, 9635709, 9699790, 9768738, 9835256, 9901408, 9970435, 10035770, 10099551, 10160590, 10222778, 10275166, 10335714, 10396792, 10470958, 10534013, 10601943, 10660766, 10724270, 10788703, 10846512, 10916377, 10976958, 11038402, 11100827, 11160400, 11224283, 11288820, 11344642, 11407294, 11469209, 11531791, 11595974, 11657650, 11718177, 11786025, 11848398, 11906328, 11969685, 12028739, 12092176, 12152096, 12216503, 12278521, 12343651, 12410253, 12467709, 12526757, 12591090, 12658189, 12721334, 12784598, 12847505, 12911302, 12969648, 13040368, 13106181, 13171372, 13234731, 13298026, 13364772, 13425974, 13486857, 13550345, 13612303, 13675168, 13746684, 13810098, 13880680, 13945907, 14008651, 14070958, 14131125, 14192525, 14255753, 14318060, 14379647, 14447024, 14507730, 14571445, 14635522, 14694060, 14763295, 14822584, 14885673, 14944967, 15018675, 15082374, 15140938, 15204153, 15257588, 15326632, 15386658, 15448145, 15511314, 15570611, 15635759, 15697611, 15756733, 15816414, 15878450, 15947966, 16002905, 16065564, 16126803, 16189482, 16249609, 16315075, 16377451, 16445525, 16516186, 16580594, 16642714, 16706097, 16770410, 16832600, 16890250, 16950669, 17013752, 17082050, 17146626, 17208533, 17272880, 17336212, 17399691, 17468712, 17527636, 17592919, 17656475, 17721918, 17792288, 17856755, 17914300, 17979696]\n",
            "2019/04/22-07:37:07 training : feed_train_211.pickle\n",
            "2019/04/22-07:37:07 training : steps = 123\n",
            "2019/04/22-07:37:27 global_steps... = 31600\n",
            "2019/04/22-07:37:56 training : step_idx = 31621\n",
            "2019/04/22-07:38:05 training : feed_train_72.pickle\n",
            "2019/04/22-07:38:05 training : steps = 118\n",
            "2019/04/22-07:39:49 global_steps... = 31700\n",
            "2019/04/22-07:40:40 training : step_idx = 31738\n",
            "2019/04/22-07:40:47 training : feed_train_122.pickle\n",
            "2019/04/22-07:40:47 training : steps = 128\n",
            "2019/04/22-07:42:09 global_steps... = 31800\n",
            "2019/04/22-07:43:38 training : step_idx = 31866\n",
            "2019/04/22-07:43:42 training : feed_train_89.pickle\n",
            "2019/04/22-07:43:42 training : steps = 122\n",
            "2019/04/22-07:44:27 global_steps... = 31900\n",
            "2019/04/22-07:45:21 progress: 9.10[%] 499.1[sec]\n",
            "2019/04/22-07:45:21 save model.ckpt\n",
            "2019/04/22-07:45:21 epoch_idx = 0, step_idx = 31941, gloal_step_idx = 31941, feed_file_no = 258\n",
            "2019/04/22-07:46:25 training : step_idx = 31988\n",
            "2019/04/22-07:46:34 training : feed_train_207.pickle\n",
            "2019/04/22-07:46:34 training : steps = 133\n",
            "2019/04/22-07:46:50 global_steps... = 32000\n",
            "2019/04/22-07:49:03 global_steps... = 32100\n",
            "2019/04/22-07:49:33 training : step_idx = 32121\n",
            "2019/04/22-07:49:38 training : feed_train_20.pickle\n",
            "2019/04/22-07:49:38 training : steps = 139\n",
            "2019/04/22-07:51:22 global_steps... = 32200\n",
            "2019/04/22-07:52:40 training : step_idx = 32259\n",
            "2019/04/22-07:52:46 training : feed_train_273.pickle\n",
            "2019/04/22-07:52:46 training : steps = 126\n",
            "2019/04/22-07:53:30 progress: 9.20[%] 489.3[sec]\n",
            "2019/04/22-07:53:30 save model.ckpt\n",
            "2019/04/22-07:53:30 epoch_idx = 0, step_idx = 32292, gloal_step_idx = 32292, feed_file_no = 261\n",
            "2019/04/22-07:53:41 global_steps... = 32300\n",
            "2019/04/22-07:55:32 training : step_idx = 32384\n",
            "2019/04/22-07:55:39 training : feed_train_117.pickle\n",
            "2019/04/22-07:55:39 training : steps = 122\n",
            "2019/04/22-07:56:00 global_steps... = 32400\n",
            "2019/04/22-07:58:12 global_steps... = 32500\n",
            "2019/04/22-07:58:20 training : step_idx = 32506\n",
            "2019/04/22-07:58:31 training : feed_train_210.pickle\n",
            "2019/04/22-07:58:31 training : steps = 124\n",
            "2019/04/22-08:00:36 global_steps... = 32600\n",
            "2019/04/22-08:01:17 training : step_idx = 32630\n",
            "2019/04/22-08:01:25 training : feed_train_64.pickle\n",
            "2019/04/22-08:01:25 training : steps = 126\n",
            "2019/04/22-08:01:42 progress: 9.30[%] 491.7[sec]\n",
            "2019/04/22-08:01:42 save model.ckpt\n",
            "2019/04/22-08:01:42 epoch_idx = 0, step_idx = 32643, gloal_step_idx = 32643, feed_file_no = 264\n",
            "2019/04/22-08:02:58 global_steps... = 32700\n",
            "2019/04/22-08:04:11 training : step_idx = 32755\n",
            "2019/04/22-08:04:16 training : feed_train_96.pickle\n",
            "2019/04/22-08:04:16 training : steps = 122\n",
            "2019/04/22-08:05:16 global_steps... = 32800\n",
            "2019/04/22-08:06:58 training : step_idx = 32877\n",
            "2019/04/22-08:07:02 training : feed_train_242.pickle\n",
            "2019/04/22-08:07:02 training : steps = 113\n",
            "2019/04/22-08:07:32 global_steps... = 32900\n",
            "2019/04/22-08:09:30 training : step_idx = 32989\n",
            "2019/04/22-08:09:34 training : feed_train_99.pickle\n",
            "2019/04/22-08:09:34 training : steps = 119\n",
            "2019/04/22-08:09:40 progress: 9.40[%] 478.4[sec]\n",
            "2019/04/22-08:09:41 save model.ckpt\n",
            "2019/04/22-08:09:41 epoch_idx = 0, step_idx = 32994, gloal_step_idx = 32994, feed_file_no = 267\n",
            "2019/04/22-08:09:49 global_steps... = 33000\n",
            "2019/04/22-08:12:01 global_steps... = 33100\n",
            "2019/04/22-08:12:10 training : step_idx = 33107\n",
            "2019/04/22-08:12:15 training : feed_train_130.pickle\n",
            "2019/04/22-08:12:15 training : steps = 124\n",
            "2019/04/22-08:14:18 global_steps... = 33200\n",
            "2019/04/22-08:14:58 training : step_idx = 33230\n",
            "2019/04/22-08:15:04 training : feed_train_24.pickle\n",
            "2019/04/22-08:15:04 training : steps = 134\n",
            "2019/04/22-08:16:37 global_steps... = 33300\n",
            "2019/04/22-08:17:36 progress: 9.50[%] 476.2[sec]\n",
            "2019/04/22-08:17:37 save model.ckpt\n",
            "2019/04/22-08:17:37 epoch_idx = 0, step_idx = 33345, gloal_step_idx = 33345, feed_file_no = 269\n",
            "2019/04/22-08:17:49 loaded feed_test.\n",
            "2019/04/22-08:22:54 train: policy=54.29559[%]  value=0.05589 test : policy=45.47095[%]  value=0.71025 \n",
            "2019/04/22-08:22:54 save model.ckpt\n",
            "2019/04/22-08:23:20 training : step_idx = 33364\n",
            "2019/04/22-08:23:25 training : feed_train_235.pickle\n",
            "2019/04/22-08:23:25 training : steps = 127\n",
            "2019/04/22-08:24:12 global_steps... = 33400\n",
            "2019/04/22-08:26:11 training : step_idx = 33490\n",
            "2019/04/22-08:26:17 training : feed_train_121.pickle\n",
            "2019/04/22-08:26:17 training : steps = 121\n",
            "2019/04/22-08:26:30 global_steps... = 33500\n",
            "2019/04/22-08:28:42 global_steps... = 33600\n",
            "2019/04/22-08:28:59 training : step_idx = 33611\n",
            "2019/04/22-08:29:05 training : feed_train_269.pickle\n",
            "2019/04/22-08:29:05 training : steps = 126\n",
            "2019/04/22-08:30:57 progress: 9.60[%] 800.8[sec]\n",
            "2019/04/22-08:30:58 save model.ckpt\n",
            "2019/04/22-08:30:58 epoch_idx = 0, step_idx = 33696, gloal_step_idx = 33696, feed_file_no = 272\n",
            "2019/04/22-08:31:03 global_steps... = 33700\n",
            "2019/04/22-08:31:53 training : step_idx = 33737\n",
            "2019/04/22-08:31:58 training : feed_train_214.pickle\n",
            "2019/04/22-08:31:58 training : steps = 124\n",
            "2019/04/22-08:33:22 global_steps... = 33800\n",
            "2019/04/22-08:34:41 training : step_idx = 33860\n",
            "2019/04/22-08:34:45 training : feed_train_73.pickle\n",
            "2019/04/22-08:34:45 training : steps = 124\n",
            "2019/04/22-08:35:38 global_steps... = 33900\n",
            "2019/04/22-08:37:32 training : step_idx = 33984\n",
            "2019/04/22-08:37:36 training : feed_train_17.pickle\n",
            "2019/04/22-08:37:36 training : steps = 135\n",
            "2019/04/22-08:37:57 global_steps... = 34000\n",
            "2019/04/22-08:38:59 progress: 9.70[%] 481.5[sec]\n",
            "2019/04/22-08:38:59 save model.ckpt\n",
            "2019/04/22-08:38:59 epoch_idx = 0, step_idx = 34047, gloal_step_idx = 34047, feed_file_no = 275\n",
            "2019/04/22-08:40:09 global_steps... = 34100\n",
            "2019/04/22-08:40:36 training : step_idx = 34119\n",
            "2019/04/22-08:40:40 training : feed_train_184.pickle\n",
            "2019/04/22-08:40:40 training : steps = 116\n",
            "2019/04/22-08:42:27 global_steps... = 34200\n",
            "2019/04/22-08:43:12 training : step_idx = 34234\n",
            "2019/04/22-08:43:19 training : feed_train_196.pickle\n",
            "2019/04/22-08:43:19 training : steps = 128\n",
            "2019/04/22-08:44:46 global_steps... = 34300\n",
            "2019/04/22-08:46:09 training : step_idx = 34362\n",
            "2019/04/22-08:46:17 training : feed_train_148.pickle\n",
            "2019/04/22-08:46:17 training : steps = 125\n",
            "2019/04/22-08:47:05 progress: 9.80[%] 486.3[sec]\n",
            "2019/04/22-08:47:06 save model.ckpt\n",
            "2019/04/22-08:47:06 epoch_idx = 0, step_idx = 34398, gloal_step_idx = 34398, feed_file_no = 278\n",
            "2019/04/22-08:47:08 global_steps... = 34400\n",
            "2019/04/22-08:49:02 training : step_idx = 34486\n",
            "2019/04/22-08:49:06 training : feed_train_233.pickle\n",
            "2019/04/22-08:49:06 training : steps = 128\n",
            "2019/04/22-08:49:25 global_steps... = 34500\n",
            "2019/04/22-08:51:37 global_steps... = 34600\n",
            "2019/04/22-08:51:57 training : step_idx = 34614\n",
            "2019/04/22-08:52:02 training : feed_train_14.pickle\n",
            "2019/04/22-08:52:02 training : steps = 138\n",
            "2019/04/22-08:53:56 global_steps... = 34700\n",
            "2019/04/22-08:55:01 progress: 9.90[%] 476.0[sec]\n",
            "2019/04/22-08:55:02 save model.ckpt\n",
            "2019/04/22-08:55:02 epoch_idx = 0, step_idx = 34749, gloal_step_idx = 34749, feed_file_no = 280\n",
            "2019/04/22-08:55:04 training : step_idx = 34751\n",
            "2019/04/22-08:55:09 training : feed_train_52.pickle\n",
            "2019/04/22-08:55:09 training : steps = 126\n",
            "2019/04/22-08:56:14 global_steps... = 34800\n",
            "2019/04/22-08:57:55 training : step_idx = 34877\n",
            "2019/04/22-08:58:00 training : feed_train_230.pickle\n",
            "2019/04/22-08:58:00 training : steps = 113\n",
            "2019/04/22-08:58:30 global_steps... = 34900\n",
            "2019/04/22-09:00:28 training : step_idx = 34989\n",
            "2019/04/22-09:00:37 training : feed_train_194.pickle\n",
            "2019/04/22-09:00:37 training : steps = 128\n",
            "2019/04/22-09:00:52 global_steps... = 35000\n",
            "2019/04/22-09:03:04 global_steps... = 35100\n",
            "2019/04/22-09:03:04 progress: 10.00[%] 483.4[sec]\n",
            "2019/04/22-09:03:05 save model.ckpt\n",
            "2019/04/22-09:03:05 epoch_idx = 0, step_idx = 35100, gloal_step_idx = 35100, feed_file_no = 283\n",
            "2019/04/22-09:03:13 loaded feed_test.\n",
            "2019/04/22-09:08:18 train: policy=51.23735[%]  value=0.05365 test : policy=45.66681[%]  value=0.73112 \n",
            "2019/04/22-09:08:18 save model.ckpt\n",
            "2019/04/22-09:08:40 shuffling...\n",
            "2019/04/22-09:08:40 feed_files_shuffled = [111, 9, 118, 244, 230, 104, 163, 33, 236, 122, 12, 25, 139, 44, 267, 186, 21, 94, 63, 110, 179, 115, 95, 77, 189, 48, 113, 116, 159, 254, 1, 224, 140, 129, 194, 172, 245, 2, 207, 117, 8, 137, 171, 271, 90, 26, 138, 66, 30, 102, 38, 196, 134, 61, 81, 148, 265, 234, 100, 55, 51, 193, 252, 249, 20, 23, 272, 242, 250, 64, 225, 98, 109, 183, 85, 73, 42, 59, 161, 97, 56, 205, 248, 75, 226, 199, 184, 169, 82, 78, 204, 4, 112, 174, 269, 273, 212, 188, 70, 133, 143, 13, 237, 36, 276, 166, 231, 152, 192, 91, 57, 247, 260, 255, 22, 3, 151, 268, 261, 50, 58, 131, 53, 65, 280, 264, 275, 155, 128, 282, 114, 84, 72, 40, 258, 164, 283, 71, 144, 222, 62, 37, 107, 47, 45, 74, 180, 181, 190, 195, 243, 239, 210, 197, 123, 256, 149, 105, 213, 175, 176, 277, 240, 279, 60, 246, 270, 96, 145, 15, 266, 154, 198, 229, 216, 126, 142, 120, 103, 83, 170, 99, 215, 209, 167, 87, 76, 150, 218, 14, 162, 34, 52, 24, 19, 80, 219, 228, 69, 41, 46, 177, 125, 232, 135, 16, 39, 141, 217, 67, 206, 156, 173, 158, 27, 32, 119, 89, 43, 106, 132, 127, 259, 178, 185, 157, 54, 124, 208, 200, 233, 168, 201, 160, 28, 68, 274, 121, 11, 187, 182, 278, 238, 257, 10, 262, 130, 220, 251, 202, 203, 86, 136, 29, 5, 88, 101, 31, 153, 35, 227, 17, 93, 7, 191, 281, 221, 263, 108, 49, 147, 92, 214, 146, 253, 165, 6, 79, 241, 18, 0, 235, 223, 211]\n",
            "2019/04/22-09:08:40 feed_n_1 = [59878, 70582, 63822, 66746, 57545, 57809, 62040, 67733, 61680, 65466, 69969, 67930, 59289, 63699, 66459, 62907, 67377, 48133, 64537, 62710, 63169, 53435, 63062, 62260, 64183, 64244, 60167, 64904, 62819, 63235, 71222, 61078, 62322, 63781, 65396, 62978, 63638, 68131, 68074, 62120, 70423, 61516, 60593, 60883, 58564, 69682, 59048, 62652, 67668, 63360, 70767, 65283, 57110, 62934, 62474, 63556, 63185, 65148, 59740, 64657, 63504, 63883, 66602, 56512, 70661, 71516, 63159, 57650, 56036, 64313, 58346, 62648, 61202, 68244, 66152, 63479, 68948, 59921, 63089, 65335, 58154, 57397, 62659, 61400, 54073, 62795, 58924, 52388, 61752, 65271, 55064, 69235, 57477, 70181, 64347, 64408, 57395, 64107, 56303, 64433, 63145, 68308, 63285, 67846, 59688, 64129, 62957, 65813, 63513, 60527, 61368, 62865, 64147, 66518, 69061, 70720, 62582, 64178, 64998, 63096, 58442, 64710, 65820, 62986, 63414, 63770, 60895, 62887, 59573, 58823, 61453, 65247, 60127, 69705, 63502, 63188, 73962, 61444, 62835, 62373, 65191, 69517, 62574, 60706, 61726, 61587, 65033, 63294, 65402, 66950, 55764, 65723, 63383, 65130, 62559, 61915, 62307, 54846, 63228, 63357, 60116, 61056, 64336, 64077, 63833, 59294, 61499, 62190, 63601, 68492, 62188, 64081, 64081, 62738, 60317, 63488, 58538, 58325, 63215, 63196, 62307, 60419, 62018, 61590, 62083, 57930, 60548, 61910, 58430, 70370, 61487, 69727, 64467, 68298, 70189, 63938, 59920, 65227, 61300, 68884, 60581, 63423, 64810, 59047, 63437, 69027, 67848, 63448, 62045, 66531, 67099, 63848, 59297, 63295, 73708, 68058, 64194, 62376, 62425, 59122, 64436, 54939, 55822, 61958, 61039, 64655, 64560, 63055, 65006, 61895, 65443, 61239, 59054, 61976, 70430, 63359, 61052, 61907, 69027, 59590, 66354, 56483, 64407, 65488, 69005, 63715, 63083, 58777, 62036, 63615, 65003, 61172, 61852, 74166, 69516, 58900, 57456, 72606, 64333, 69044, 62744, 69021, 55255, 69865, 63797, 63222, 59681, 64936, 60026, 60150, 65064, 63264, 63332, 62456, 66874, 60614, 68350, 62889, 61676, 70825, 68940, 64576, 59858, 62679]\n",
            "2019/04/22-09:08:40 feed_n_shuffled = [59878, 130460, 194282, 261028, 318573, 376382, 438422, 506155, 567835, 633301, 703270, 771200, 830489, 894188, 960647, 1023554, 1090931, 1139064, 1203601, 1266311, 1329480, 1382915, 1445977, 1508237, 1572420, 1636664, 1696831, 1761735, 1824554, 1887789, 1959011, 2020089, 2082411, 2146192, 2211588, 2274566, 2338204, 2406335, 2474409, 2536529, 2606952, 2668468, 2729061, 2789944, 2848508, 2918190, 2977238, 3039890, 3107558, 3170918, 3241685, 3306968, 3364078, 3427012, 3489486, 3553042, 3616227, 3681375, 3741115, 3805772, 3869276, 3933159, 3999761, 4056273, 4126934, 4198450, 4261609, 4319259, 4375295, 4439608, 4497954, 4560602, 4621804, 4690048, 4756200, 4819679, 4888627, 4948548, 5011637, 5076972, 5135126, 5192523, 5255182, 5316582, 5370655, 5433450, 5492374, 5544762, 5606514, 5671785, 5726849, 5796084, 5853561, 5923742, 5988089, 6052497, 6109892, 6173999, 6230302, 6294735, 6357880, 6426188, 6489473, 6557319, 6617007, 6681136, 6744093, 6809906, 6873419, 6933946, 6995314, 7058179, 7122326, 7188844, 7257905, 7328625, 7391207, 7455385, 7520383, 7583479, 7641921, 7706631, 7772451, 7835437, 7898851, 7962621, 8023516, 8086403, 8145976, 8204799, 8266252, 8331499, 8391626, 8461331, 8524833, 8588021, 8661983, 8723427, 8786262, 8848635, 8913826, 8983343, 9045917, 9106623, 9168349, 9229936, 9294969, 9358263, 9423665, 9490615, 9546379, 9612102, 9675485, 9740615, 9803174, 9865089, 9927396, 9982242, 10045470, 10108827, 10168943, 10229999, 10294335, 10358412, 10422245, 10481539, 10543038, 10605228, 10668829, 10737321, 10799509, 10863590, 10927671, 10990409, 11050726, 11114214, 11172752, 11231077, 11294292, 11357488, 11419795, 11480214, 11542232, 11603822, 11665905, 11723835, 11784383, 11846293, 11904723, 11975093, 12036580, 12106307, 12170774, 12239072, 12309261, 12373199, 12433119, 12498346, 12559646, 12628530, 12689111, 12752534, 12817344, 12876391, 12939828, 13008855, 13076703, 13140151, 13202196, 13268727, 13335826, 13399674, 13458971, 13522266, 13595974, 13664032, 13728226, 13790602, 13853027, 13912149, 13976585, 14031524, 14087346, 14149304, 14210343, 14274998, 14339558, 14402613, 14467619, 14529514, 14594957, 14656196, 14715250, 14777226, 14847656, 14911015, 14972067, 15033974, 15103001, 15162591, 15228945, 15285428, 15349835, 15415323, 15484328, 15548043, 15611126, 15669903, 15731939, 15795554, 15860557, 15921729, 15983581, 16057747, 16127263, 16186163, 16243619, 16316225, 16380558, 16449602, 16512346, 16581367, 16636622, 16706487, 16770284, 16833506, 16893187, 16958123, 17018149, 17078299, 17143363, 17206627, 17269959, 17332415, 17399289, 17459903, 17528253, 17591142, 17652818, 17723643, 17792583, 17857159, 17917017, 17979696]\n",
            "2019/04/22-09:08:46 training : feed_train_111.pickle\n",
            "2019/04/22-09:08:46 training : steps = 117\n",
            "2019/04/22-09:10:37 global_steps... = 35200\n",
            "2019/04/22-09:11:23 training : step_idx = 117\n",
            "2019/04/22-09:11:27 training : feed_train_9.pickle\n",
            "2019/04/22-09:11:27 training : steps = 138\n",
            "2019/04/22-09:12:56 global_steps... = 35300\n",
            "2019/04/22-09:14:32 training : step_idx = 255\n",
            "2019/04/22-09:14:45 training : feed_train_118.pickle\n",
            "2019/04/22-09:14:45 training : steps = 125\n",
            "2019/04/22-09:15:24 global_steps... = 35400\n",
            "2019/04/22-09:16:31 progress: 10.10[%] 806.1[sec]\n",
            "2019/04/22-09:16:31 save model.ckpt\n",
            "2019/04/22-09:16:31 epoch_idx = 1, step_idx = 335, gloal_step_idx = 35451, feed_file_no = 2\n",
            "2019/04/22-09:17:32 training : step_idx = 380\n",
            "2019/04/22-09:17:36 training : feed_train_244.pickle\n",
            "2019/04/22-09:17:36 training : steps = 131\n",
            "2019/04/22-09:17:42 global_steps... = 35500\n",
            "2019/04/22-09:19:54 global_steps... = 35600\n",
            "2019/04/22-09:20:28 training : step_idx = 510\n",
            "2019/04/22-09:20:31 training : feed_train_230.pickle\n",
            "2019/04/22-09:20:31 training : steps = 113\n",
            "2019/04/22-09:22:09 global_steps... = 35700\n",
            "2019/04/22-09:23:00 training : step_idx = 623\n",
            "2019/04/22-09:23:04 training : feed_train_104.pickle\n",
            "2019/04/22-09:23:04 training : steps = 113\n",
            "2019/04/22-09:24:25 global_steps... = 35800\n",
            "2019/04/22-09:24:27 progress: 10.20[%] 476.7[sec]\n",
            "2019/04/22-09:24:28 save model.ckpt\n",
            "2019/04/22-09:24:28 epoch_idx = 1, step_idx = 686, gloal_step_idx = 35802, feed_file_no = 5\n",
            "2019/04/22-09:25:36 training : step_idx = 736\n",
            "2019/04/22-09:25:40 training : feed_train_163.pickle\n",
            "2019/04/22-09:25:40 training : steps = 122\n",
            "2019/04/22-09:26:43 global_steps... = 35900\n",
            "2019/04/22-09:28:20 training : step_idx = 857\n",
            "2019/04/22-09:28:26 training : feed_train_33.pickle\n",
            "2019/04/22-09:28:26 training : steps = 133\n",
            "2019/04/22-09:29:01 global_steps... = 36000\n",
            "2019/04/22-09:31:13 global_steps... = 36100\n",
            "2019/04/22-09:31:20 training : step_idx = 989\n",
            "2019/04/22-09:31:24 training : feed_train_236.pickle\n",
            "2019/04/22-09:31:24 training : steps = 121\n",
            "2019/04/22-09:32:28 progress: 10.30[%] 480.6[sec]\n",
            "2019/04/22-09:32:28 save model.ckpt\n",
            "2019/04/22-09:32:28 epoch_idx = 1, step_idx = 1037, gloal_step_idx = 36153, feed_file_no = 8\n",
            "2019/04/22-09:33:31 global_steps... = 36200\n",
            "2019/04/22-09:34:05 training : step_idx = 1110\n",
            "2019/04/22-09:34:08 training : feed_train_122.pickle\n",
            "2019/04/22-09:34:08 training : steps = 128\n",
            "2019/04/22-09:35:46 global_steps... = 36300\n",
            "2019/04/22-09:36:56 training : step_idx = 1237\n",
            "2019/04/22-09:37:00 training : feed_train_12.pickle\n",
            "2019/04/22-09:37:00 training : steps = 137\n",
            "2019/04/22-09:38:03 global_steps... = 36400\n",
            "2019/04/22-09:40:03 training : step_idx = 1374\n",
            "2019/04/22-09:40:15 training : feed_train_25.pickle\n",
            "2019/04/22-09:40:15 training : steps = 133\n",
            "2019/04/22-09:40:28 global_steps... = 36500\n",
            "2019/04/22-09:40:33 progress: 10.40[%] 485.3[sec]\n",
            "2019/04/22-09:40:34 save model.ckpt\n",
            "2019/04/22-09:40:34 epoch_idx = 1, step_idx = 1388, gloal_step_idx = 36504, feed_file_no = 11\n",
            "2019/04/22-09:42:41 global_steps... = 36600\n",
            "2019/04/22-09:43:13 training : step_idx = 1507\n",
            "2019/04/22-09:43:16 training : feed_train_139.pickle\n",
            "2019/04/22-09:43:16 training : steps = 116\n",
            "2019/04/22-09:44:59 global_steps... = 36700\n",
            "2019/04/22-09:45:52 training : step_idx = 1623\n",
            "2019/04/22-09:45:56 training : feed_train_44.pickle\n",
            "2019/04/22-09:45:56 training : steps = 125\n",
            "2019/04/22-09:47:16 global_steps... = 36800\n",
            "2019/04/22-09:48:29 progress: 10.50[%] 475.7[sec]\n",
            "2019/04/22-09:48:30 save model.ckpt\n",
            "2019/04/22-09:48:30 epoch_idx = 1, step_idx = 1739, gloal_step_idx = 36855, feed_file_no = 13\n",
            "2019/04/22-09:48:38 loaded feed_test.\n",
            "2019/04/22-09:53:42 train: policy=52.00194[%]  value=0.04861 test : policy=45.75319[%]  value=0.72906 \n",
            "2019/04/22-09:53:42 save model.ckpt\n",
            "2019/04/22-09:53:53 training : step_idx = 1747\n",
            "2019/04/22-09:53:57 training : feed_train_267.pickle\n",
            "2019/04/22-09:53:57 training : steps = 130\n",
            "2019/04/22-09:54:46 global_steps... = 36900\n",
            "2019/04/22-09:56:50 training : step_idx = 1877\n",
            "2019/04/22-09:56:57 training : feed_train_186.pickle\n",
            "2019/04/22-09:56:57 training : steps = 123\n",
            "2019/04/22-09:57:06 global_steps... = 37000\n",
            "2019/04/22-09:59:19 global_steps... = 37100\n",
            "2019/04/22-09:59:42 training : step_idx = 2000\n",
            "2019/04/22-09:59:47 training : feed_train_21.pickle\n",
            "2019/04/22-09:59:47 training : steps = 132\n",
            "2019/04/22-10:01:37 global_steps... = 37200\n",
            "2019/04/22-10:01:45 progress: 10.60[%] 796.3[sec]\n",
            "2019/04/22-10:01:46 save model.ckpt\n",
            "2019/04/22-10:01:46 epoch_idx = 1, step_idx = 2090, gloal_step_idx = 37206, feed_file_no = 16\n",
            "2019/04/22-10:02:40 training : step_idx = 2131\n",
            "2019/04/22-10:02:44 training : feed_train_94.pickle\n",
            "2019/04/22-10:02:44 training : steps = 95\n",
            "2019/04/22-10:03:54 global_steps... = 37300\n",
            "2019/04/22-10:04:48 training : step_idx = 2225\n",
            "2019/04/22-10:04:52 training : feed_train_63.pickle\n",
            "2019/04/22-10:04:52 training : steps = 127\n",
            "2019/04/22-10:06:10 global_steps... = 37400\n",
            "2019/04/22-10:07:39 training : step_idx = 2351\n",
            "2019/04/22-10:07:43 training : feed_train_110.pickle\n",
            "2019/04/22-10:07:43 training : steps = 123\n",
            "2019/04/22-10:08:26 global_steps... = 37500\n",
            "2019/04/22-10:09:42 progress: 10.70[%] 476.3[sec]\n",
            "2019/04/22-10:09:42 save model.ckpt\n",
            "2019/04/22-10:09:42 epoch_idx = 1, step_idx = 2441, gloal_step_idx = 37557, feed_file_no = 19\n",
            "2019/04/22-10:10:26 training : step_idx = 2474\n",
            "2019/04/22-10:10:31 training : feed_train_179.pickle\n",
            "2019/04/22-10:10:31 training : steps = 124\n",
            "2019/04/22-10:10:44 global_steps... = 37600\n",
            "2019/04/22-10:12:56 global_steps... = 37700\n",
            "2019/04/22-10:13:14 training : step_idx = 2597\n",
            "2019/04/22-10:13:20 training : feed_train_115.pickle\n",
            "2019/04/22-10:13:20 training : steps = 105\n",
            "2019/04/22-10:15:15 global_steps... = 37800\n",
            "2019/04/22-10:15:39 training : step_idx = 2702\n",
            "2019/04/22-10:15:43 training : feed_train_95.pickle\n",
            "2019/04/22-10:15:43 training : steps = 124\n",
            "2019/04/22-10:17:32 global_steps... = 37900\n",
            "2019/04/22-10:17:42 progress: 10.80[%] 480.8[sec]\n",
            "2019/04/22-10:17:43 save model.ckpt\n",
            "2019/04/22-10:17:43 epoch_idx = 1, step_idx = 2792, gloal_step_idx = 37908, feed_file_no = 22\n",
            "2019/04/22-10:18:26 training : step_idx = 2825\n",
            "2019/04/22-10:18:30 training : feed_train_77.pickle\n",
            "2019/04/22-10:18:30 training : steps = 122\n",
            "2019/04/22-10:19:48 global_steps... = 38000\n",
            "2019/04/22-10:21:10 training : step_idx = 2946\n",
            "2019/04/22-10:21:18 training : feed_train_189.pickle\n",
            "2019/04/22-10:21:18 training : steps = 126\n",
            "2019/04/22-10:22:08 global_steps... = 38100\n",
            "2019/04/22-10:24:05 training : step_idx = 3072\n",
            "2019/04/22-10:24:09 training : feed_train_48.pickle\n",
            "2019/04/22-10:24:09 training : steps = 126\n",
            "2019/04/22-10:24:25 global_steps... = 38200\n",
            "2019/04/22-10:25:43 progress: 10.90[%] 480.6[sec]\n",
            "2019/04/22-10:25:43 save model.ckpt\n",
            "2019/04/22-10:25:43 epoch_idx = 1, step_idx = 3143, gloal_step_idx = 38259, feed_file_no = 25\n",
            "2019/04/22-10:26:38 global_steps... = 38300\n",
            "2019/04/22-10:26:55 training : step_idx = 3197\n",
            "2019/04/22-10:26:58 training : feed_train_113.pickle\n",
            "2019/04/22-10:26:58 training : steps = 118\n",
            "2019/04/22-10:28:53 global_steps... = 38400\n",
            "2019/04/22-10:29:35 training : step_idx = 3315\n",
            "2019/04/22-10:29:42 training : feed_train_116.pickle\n",
            "2019/04/22-10:29:42 training : steps = 127\n",
            "2019/04/22-10:31:13 global_steps... = 38500\n",
            "2019/04/22-10:32:28 training : step_idx = 3441\n",
            "2019/04/22-10:32:35 training : feed_train_159.pickle\n",
            "2019/04/22-10:32:35 training : steps = 123\n",
            "2019/04/22-10:33:32 global_steps... = 38600\n",
            "2019/04/22-10:33:45 progress: 10.99[%] 482.5[sec]\n",
            "2019/04/22-10:33:46 save model.ckpt\n",
            "2019/04/22-10:33:46 epoch_idx = 1, step_idx = 3494, gloal_step_idx = 38610, feed_file_no = 28\n",
            "2019/04/22-10:33:54 loaded feed_test.\n",
            "2019/04/22-10:39:00 train: policy=48.31883[%]  value=0.08595 test : policy=45.94410[%]  value=0.65460 \n",
            "2019/04/22-10:39:00 save model.ckpt\n",
            "2019/04/22-10:40:34 training : step_idx = 3564\n",
            "2019/04/22-10:40:39 training : feed_train_254.pickle\n",
            "2019/04/22-10:40:39 training : steps = 124\n",
            "2019/04/22-10:41:06 global_steps... = 38700\n",
            "2019/04/22-10:43:18 global_steps... = 38800\n",
            "2019/04/22-10:43:23 training : step_idx = 3688\n",
            "2019/04/22-10:43:27 training : feed_train_1.pickle\n",
            "2019/04/22-10:43:27 training : steps = 140\n",
            "2019/04/22-10:45:35 global_steps... = 38900\n",
            "2019/04/22-10:46:32 training : step_idx = 3827\n",
            "2019/04/22-10:46:39 training : feed_train_224.pickle\n",
            "2019/04/22-10:46:39 training : steps = 120\n",
            "2019/04/22-10:47:03 progress: 11.09[%] 797.3[sec]\n",
            "2019/04/22-10:47:03 save model.ckpt\n",
            "2019/04/22-10:47:03 epoch_idx = 1, step_idx = 3845, gloal_step_idx = 38961, feed_file_no = 31\n",
            "2019/04/22-10:47:55 global_steps... = 39000\n",
            "2019/04/22-10:49:17 training : step_idx = 3946\n",
            "2019/04/22-10:49:24 training : feed_train_140.pickle\n",
            "2019/04/22-10:49:24 training : steps = 122\n",
            "2019/04/22-10:50:15 global_steps... = 39100\n",
            "2019/04/22-10:52:07 training : step_idx = 4068\n",
            "2019/04/22-10:52:14 training : feed_train_129.pickle\n",
            "2019/04/22-10:52:14 training : steps = 125\n",
            "2019/04/22-10:52:35 global_steps... = 39200\n",
            "2019/04/22-10:54:48 global_steps... = 39300\n",
            "2019/04/22-10:54:59 training : step_idx = 4192\n",
            "2019/04/22-10:55:01 training : feed_train_194.pickle\n",
            "2019/04/22-10:55:01 training : steps = 128\n",
            "2019/04/22-10:55:07 progress: 11.19[%] 483.8[sec]\n",
            "2019/04/22-10:55:07 save model.ckpt\n",
            "2019/04/22-10:55:07 epoch_idx = 1, step_idx = 4196, gloal_step_idx = 39312, feed_file_no = 34\n",
            "2019/04/22-10:57:04 global_steps... = 39400\n",
            "2019/04/22-10:57:52 training : step_idx = 4320\n",
            "2019/04/22-10:57:59 training : feed_train_172.pickle\n",
            "2019/04/22-10:57:59 training : steps = 124\n",
            "2019/04/22-10:59:24 global_steps... = 39500\n",
            "2019/04/22-11:00:42 training : step_idx = 4443\n",
            "2019/04/22-11:00:50 training : feed_train_245.pickle\n",
            "2019/04/22-11:00:50 training : steps = 125\n",
            "2019/04/22-11:01:45 global_steps... = 39600\n",
            "2019/04/22-11:03:08 progress: 11.29[%] 481.6[sec]\n",
            "2019/04/22-11:03:09 save model.ckpt\n",
            "2019/04/22-11:03:09 epoch_idx = 1, step_idx = 4547, gloal_step_idx = 39663, feed_file_no = 36\n",
            "2019/04/22-11:03:35 training : step_idx = 4567\n",
            "2019/04/22-11:03:40 training : feed_train_2.pickle\n",
            "2019/04/22-11:03:40 training : steps = 134\n",
            "2019/04/22-11:04:02 global_steps... = 39700\n",
            "2019/04/22-11:06:15 global_steps... = 39800\n",
            "2019/04/22-11:06:36 training : step_idx = 4700\n",
            "2019/04/22-11:06:39 training : feed_train_207.pickle\n",
            "2019/04/22-11:06:39 training : steps = 133\n",
            "2019/04/22-11:08:30 global_steps... = 39900\n",
            "2019/04/22-11:09:35 training : step_idx = 4833\n",
            "2019/04/22-11:09:37 training : feed_train_117.pickle\n",
            "2019/04/22-11:09:37 training : steps = 122\n",
            "2019/04/22-11:10:45 global_steps... = 40000\n",
            "2019/04/22-11:11:03 progress: 11.39[%] 475.1[sec]\n",
            "2019/04/22-11:11:04 save model.ckpt\n",
            "2019/04/22-11:11:04 epoch_idx = 1, step_idx = 4898, gloal_step_idx = 40014, feed_file_no = 39\n",
            "2019/04/22-11:12:18 training : step_idx = 4955\n",
            "2019/04/22-11:12:23 training : feed_train_8.pickle\n",
            "2019/04/22-11:12:23 training : steps = 138\n",
            "2019/04/22-11:13:02 global_steps... = 40100\n",
            "2019/04/22-11:15:14 global_steps... = 40200\n",
            "2019/04/22-11:15:25 training : step_idx = 5092\n",
            "2019/04/22-11:15:30 training : feed_train_137.pickle\n",
            "2019/04/22-11:15:30 training : steps = 121\n",
            "2019/04/22-11:17:32 global_steps... = 40300\n",
            "2019/04/22-11:18:09 training : step_idx = 5212\n",
            "2019/04/22-11:18:22 training : feed_train_171.pickle\n",
            "2019/04/22-11:18:22 training : steps = 119\n",
            "2019/04/22-11:19:12 progress: 11.49[%] 488.3[sec]\n",
            "2019/04/22-11:19:12 save model.ckpt\n",
            "2019/04/22-11:19:12 epoch_idx = 1, step_idx = 5249, gloal_step_idx = 40365, feed_file_no = 42\n",
            "2019/04/22-11:19:20 loaded feed_test.\n",
            "2019/04/22-11:24:24 train: policy=47.85983[%]  value=0.11581 test : policy=45.96666[%]  value=0.70292 \n",
            "2019/04/22-11:24:24 save model.ckpt\n",
            "2019/04/22-11:25:11 global_steps... = 40400\n",
            "2019/04/22-11:26:13 training : step_idx = 5331\n",
            "2019/04/22-11:26:17 training : feed_train_271.pickle\n",
            "2019/04/22-11:26:17 training : steps = 119\n",
            "2019/04/22-11:27:27 global_steps... = 40500\n",
            "2019/04/22-11:28:55 training : step_idx = 5450\n",
            "2019/04/22-11:28:59 training : feed_train_90.pickle\n",
            "2019/04/22-11:28:59 training : steps = 115\n",
            "2019/04/22-11:29:44 global_steps... = 40600\n",
            "2019/04/22-11:31:30 training : step_idx = 5564\n",
            "2019/04/22-11:31:35 training : feed_train_26.pickle\n",
            "2019/04/22-11:31:35 training : steps = 137\n",
            "2019/04/22-11:32:02 global_steps... = 40700\n",
            "2019/04/22-11:32:23 progress: 11.59[%] 791.8[sec]\n",
            "2019/04/22-11:32:24 save model.ckpt\n",
            "2019/04/22-11:32:24 epoch_idx = 1, step_idx = 5600, gloal_step_idx = 40716, feed_file_no = 45\n",
            "2019/04/22-11:34:15 global_steps... = 40800\n",
            "2019/04/22-11:34:36 training : step_idx = 5700\n",
            "2019/04/22-11:34:40 training : feed_train_138.pickle\n",
            "2019/04/22-11:34:40 training : steps = 116\n",
            "2019/04/22-11:36:31 global_steps... = 40900\n",
            "2019/04/22-11:37:12 training : step_idx = 5815\n",
            "2019/04/22-11:37:17 training : feed_train_66.pickle\n",
            "2019/04/22-11:37:17 training : steps = 123\n",
            "2019/04/22-11:38:48 global_steps... = 41000\n",
            "2019/04/22-11:40:00 training : step_idx = 5938\n",
            "2019/04/22-11:40:06 training : feed_train_30.pickle\n",
            "2019/04/22-11:40:06 training : steps = 133\n",
            "2019/04/22-11:40:23 progress: 11.69[%] 480.1[sec]\n",
            "2019/04/22-11:40:24 save model.ckpt\n",
            "2019/04/22-11:40:24 epoch_idx = 1, step_idx = 5951, gloal_step_idx = 41067, feed_file_no = 48\n",
            "2019/04/22-11:41:08 global_steps... = 41100\n",
            "2019/04/22-11:43:02 training : step_idx = 6070\n",
            "2019/04/22-11:43:06 training : feed_train_102.pickle\n",
            "2019/04/22-11:43:06 training : steps = 124\n",
            "2019/04/22-11:43:25 global_steps... = 41200\n",
            "2019/04/22-11:45:37 global_steps... = 41300\n",
            "2019/04/22-11:45:52 training : step_idx = 6194\n",
            "2019/04/22-11:45:58 training : feed_train_38.pickle\n",
            "2019/04/22-11:45:58 training : steps = 139\n",
            "2019/04/22-11:47:57 global_steps... = 41400\n",
            "2019/04/22-11:48:21 progress: 11.79[%] 477.7[sec]\n",
            "2019/04/22-11:48:22 save model.ckpt\n",
            "2019/04/22-11:48:22 epoch_idx = 1, step_idx = 6302, gloal_step_idx = 41418, feed_file_no = 50\n",
            "2019/04/22-11:49:02 training : step_idx = 6332\n",
            "2019/04/22-11:49:04 training : feed_train_196.pickle\n",
            "2019/04/22-11:49:04 training : steps = 128\n",
            "2019/04/22-11:50:13 global_steps... = 41500\n",
            "2019/04/22-11:51:52 training : step_idx = 6459\n",
            "2019/04/22-11:51:58 training : feed_train_134.pickle\n",
            "2019/04/22-11:51:58 training : steps = 112\n",
            "2019/04/22-11:52:31 global_steps... = 41600\n",
            "2019/04/22-11:54:27 training : step_idx = 6571\n",
            "2019/04/22-11:54:31 training : feed_train_61.pickle\n",
            "2019/04/22-11:54:31 training : steps = 123\n",
            "2019/04/22-11:54:48 global_steps... = 41700\n",
            "2019/04/22-11:56:19 progress: 11.89[%] 478.3[sec]\n",
            "2019/04/22-11:56:20 save model.ckpt\n",
            "2019/04/22-11:56:20 epoch_idx = 1, step_idx = 6653, gloal_step_idx = 41769, feed_file_no = 53\n",
            "2019/04/22-11:57:01 global_steps... = 41800\n",
            "2019/04/22-11:57:16 training : step_idx = 6694\n",
            "2019/04/22-11:57:20 training : feed_train_81.pickle\n",
            "2019/04/22-11:57:20 training : steps = 123\n",
            "2019/04/22-11:59:19 global_steps... = 41900\n",
            "2019/04/22-12:00:01 training : step_idx = 6816\n",
            "2019/04/22-12:00:04 training : feed_train_148.pickle\n",
            "2019/04/22-12:00:04 training : steps = 125\n",
            "2019/04/22-12:01:34 global_steps... = 42000\n",
            "2019/04/22-12:02:48 training : step_idx = 6940\n",
            "2019/04/22-12:02:52 training : feed_train_265.pickle\n",
            "2019/04/22-12:02:52 training : steps = 124\n",
            "2019/04/22-12:03:51 global_steps... = 42100\n",
            "2019/04/22-12:04:17 progress: 11.99[%] 477.5[sec]\n",
            "2019/04/22-12:04:18 save model.ckpt\n",
            "2019/04/22-12:04:18 epoch_idx = 1, step_idx = 7004, gloal_step_idx = 42120, feed_file_no = 56\n",
            "2019/04/22-12:04:26 loaded feed_test.\n",
            "2019/04/22-12:09:30 train: policy=49.36140[%]  value=0.07680 test : policy=45.50231[%]  value=0.72002 \n",
            "2019/04/22-12:09:30 save model.ckpt\n",
            "2019/04/22-12:10:49 training : step_idx = 7063\n",
            "2019/04/22-12:10:54 training : feed_train_234.pickle\n",
            "2019/04/22-12:10:54 training : steps = 128\n",
            "2019/04/22-12:11:21 global_steps... = 42200\n",
            "2019/04/22-12:13:33 global_steps... = 42300\n",
            "2019/04/22-12:13:42 training : step_idx = 7191\n",
            "2019/04/22-12:13:47 training : feed_train_100.pickle\n",
            "2019/04/22-12:13:47 training : steps = 117\n",
            "2019/04/22-12:15:49 global_steps... = 42400\n",
            "2019/04/22-12:16:20 training : step_idx = 7307\n",
            "2019/04/22-12:16:24 training : feed_train_55.pickle\n",
            "2019/04/22-12:16:24 training : steps = 127\n",
            "2019/04/22-12:17:27 progress: 12.09[%] 790.5[sec]\n",
            "2019/04/22-12:17:28 save model.ckpt\n",
            "2019/04/22-12:17:28 epoch_idx = 1, step_idx = 7355, gloal_step_idx = 42471, feed_file_no = 59\n",
            "2019/04/22-12:18:06 global_steps... = 42500\n",
            "2019/04/22-12:19:12 training : step_idx = 7434\n",
            "2019/04/22-12:19:16 training : feed_train_51.pickle\n",
            "2019/04/22-12:19:16 training : steps = 125\n",
            "2019/04/22-12:20:22 global_steps... = 42600\n",
            "2019/04/22-12:21:59 training : step_idx = 7558\n",
            "2019/04/22-12:22:03 training : feed_train_193.pickle\n",
            "2019/04/22-12:22:03 training : steps = 125\n",
            "2019/04/22-12:22:38 global_steps... = 42700\n",
            "2019/04/22-12:24:47 training : step_idx = 7682\n",
            "2019/04/22-12:24:52 training : feed_train_252.pickle\n",
            "2019/04/22-12:24:52 training : steps = 131\n",
            "2019/04/22-12:24:55 global_steps... = 42800\n",
            "2019/04/22-12:25:24 progress: 12.19[%] 476.7[sec]\n",
            "2019/04/22-12:25:25 save model.ckpt\n",
            "2019/04/22-12:25:25 epoch_idx = 1, step_idx = 7706, gloal_step_idx = 42822, feed_file_no = 62\n",
            "2019/04/22-12:27:08 global_steps... = 42900\n",
            "2019/04/22-12:27:45 training : step_idx = 7813\n",
            "2019/04/22-12:27:50 training : feed_train_249.pickle\n",
            "2019/04/22-12:27:50 training : steps = 111\n",
            "2019/04/22-12:29:23 global_steps... = 43000\n",
            "2019/04/22-12:30:15 training : step_idx = 7923\n",
            "2019/04/22-12:30:18 training : feed_train_20.pickle\n",
            "2019/04/22-12:30:18 training : steps = 139\n",
            "2019/04/22-12:31:38 global_steps... = 43100\n",
            "2019/04/22-12:33:15 progress: 12.29[%] 470.4[sec]\n",
            "2019/04/22-12:33:15 save model.ckpt\n",
            "2019/04/22-12:33:15 epoch_idx = 1, step_idx = 8057, gloal_step_idx = 43173, feed_file_no = 64\n",
            "2019/04/22-12:33:21 training : step_idx = 8061\n",
            "2019/04/22-12:33:26 training : feed_train_23.pickle\n",
            "2019/04/22-12:33:26 training : steps = 140\n",
            "2019/04/22-12:33:56 global_steps... = 43200\n",
            "2019/04/22-12:36:09 global_steps... = 43300\n",
            "2019/04/22-12:36:32 training : step_idx = 8201\n",
            "2019/04/22-12:36:36 training : feed_train_272.pickle\n",
            "2019/04/22-12:36:36 training : steps = 124\n",
            "2019/04/22-12:38:26 global_steps... = 43400\n",
            "2019/04/22-12:39:19 training : step_idx = 8324\n",
            "2019/04/22-12:39:21 training : feed_train_242.pickle\n",
            "2019/04/22-12:39:21 training : steps = 113\n",
            "2019/04/22-12:40:41 global_steps... = 43500\n",
            "2019/04/22-12:41:12 progress: 12.39[%] 477.8[sec]\n",
            "2019/04/22-12:41:13 save model.ckpt\n",
            "2019/04/22-12:41:13 epoch_idx = 1, step_idx = 8408, gloal_step_idx = 43524, feed_file_no = 67\n",
            "2019/04/22-12:41:52 training : step_idx = 8437\n",
            "2019/04/22-12:41:56 training : feed_train_250.pickle\n",
            "2019/04/22-12:41:56 training : steps = 110\n",
            "2019/04/22-12:42:58 global_steps... = 43600\n",
            "2019/04/22-12:44:20 training : step_idx = 8546\n",
            "2019/04/22-12:44:23 training : feed_train_64.pickle\n",
            "2019/04/22-12:44:23 training : steps = 126\n",
            "2019/04/22-12:45:13 global_steps... = 43700\n",
            "2019/04/22-12:47:10 training : step_idx = 8672\n",
            "2019/04/22-12:47:15 training : feed_train_225.pickle\n",
            "2019/04/22-12:47:15 training : steps = 114\n",
            "2019/04/22-12:47:31 global_steps... = 43800\n",
            "2019/04/22-12:49:10 progress: 12.49[%] 477.4[sec]\n",
            "2019/04/22-12:49:10 save model.ckpt\n",
            "2019/04/22-12:49:10 epoch_idx = 1, step_idx = 8759, gloal_step_idx = 43875, feed_file_no = 70\n",
            "2019/04/22-12:49:18 loaded feed_test.\n",
            "2019/04/22-12:54:23 train: policy=52.77021[%]  value=0.06548 test : policy=46.27476[%]  value=0.70802 \n",
            "2019/04/22-12:54:23 save model.ckpt\n",
            "2019/04/22-12:54:57 global_steps... = 43900\n",
            "2019/04/22-12:55:00 training : step_idx = 8786\n",
            "2019/04/22-12:55:08 training : feed_train_98.pickle\n",
            "2019/04/22-12:55:08 training : steps = 123\n",
            "2019/04/22-12:57:18 global_steps... = 44000\n",
            "2019/04/22-12:57:50 training : step_idx = 8908\n",
            "2019/04/22-12:57:54 training : feed_train_109.pickle\n",
            "2019/04/22-12:57:54 training : steps = 120\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}